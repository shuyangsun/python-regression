{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This Python script is to do linear or logistic regression, using gradient descent or normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "# Scientific Calculation Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Built-in Module Imports\n",
    "import math\n",
    "import time\n",
    "import enum\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression Type\n",
    "class RegressionType(enum.Enum):\n",
    "    linear = 1\n",
    "    logistic = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Algorithm Class\n",
    "class RegressionAlgorithm(enum.Enum):\n",
    "    unspecified = 0\n",
    "    gradient_descent = 1\n",
    "    normal_equation = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataProcessor():\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_x0_column(A):\n",
    "        return np.insert(A, obj=0, values=1, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def augmented_to_coefficient_and_b(A):\n",
    "        return (A[:, :-1], A[:, -1])\n",
    "\n",
    "    @staticmethod\n",
    "    def partition(A, atInd):\n",
    "        return (A[:atInd], A[atInd:])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_unique_categories(output, case_sensitive=True):\n",
    "        if not case_sensitive:\n",
    "            output = [x.lower() if isinstance(x, str) else x for x in output]\n",
    "        return np.unique(output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_unique_categories_and_binary_outputs(output, case_sensitive=True):\n",
    "        unique_cat = DataProcessor.get_unique_categories(output, case_sensitive)\n",
    "\n",
    "        if np.size(unique_cat) <= 2:\n",
    "            outputs_b = np.zeros(np.size(output))\n",
    "            mask_0 = (output != unique_cat[0])\n",
    "            mask_1 = (output == unique_cat[0])\n",
    "            outputs_b[mask_0] = 0\n",
    "            outputs_b[mask_1] = 1\n",
    "        else:\n",
    "            outputs_b = np.tile(output, (np.size(unique_cat), 1))\n",
    "            for i, cat in enumerate(unique_cat):\n",
    "                row = outputs_b[i]\n",
    "                mask_0 = (output != cat)\n",
    "                mask_1 = (output == cat)\n",
    "                row[mask_0] = 0\n",
    "                row[mask_1] = 1\n",
    "        \n",
    "        return (unique_cat, outputs_b)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataScalar():\n",
    "    \n",
    "    def __init__(self, data, data_has_x0_column=False):\n",
    "        self.__data = data.astype(np.float64)\n",
    "        self.__data_has_x0_column = data_has_x0_column \n",
    "        self.__scalars = np.ones(np.size(data, axis=1))\n",
    "        self.__calculate_scalars()\n",
    "    \n",
    "    def scaled_data(self):\n",
    "        return self.scale_new_data(self.__data, self.__data_has_x0_column)\n",
    "    \n",
    "    def scale_new_data(self, data, input_has_x0_column=False):\n",
    "        if input_has_x0_column:\n",
    "            avg = np.insert(self.__avg, obj=0, values=0)\n",
    "            std = np.insert(self.__std, obj=0, values=1)\n",
    "        else:\n",
    "            avg = self.__avg\n",
    "            std = self.__std\n",
    "        return (data - avg) / std\n",
    "        \n",
    "    def __calculate_scalars(self):\n",
    "        if self.__data_has_x0_column:\n",
    "            self.__avg = np.average(self.__data[:, 1:], axis=0)\n",
    "            self.__std = np.std(self.__data[:, 1:], axis=0)\n",
    "        else:\n",
    "            self.__avg = np.average(self.__data, axis=0)\n",
    "            self.__std = np.std(self.__data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainer(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    def __init__(self, coefficient_matrix, outputs, regularization_lambda=0.0):\n",
    "        self._x = coefficient_matrix\n",
    "        self._y = outputs\n",
    "        self._regularization_lambda = regularization_lambda\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self._theta\n",
    "\n",
    "    def start_training(self,\n",
    "                       training_algorithm=RegressionAlgorithm.unspecified,\n",
    "                       learning_rate=0.01,\n",
    "                       time_limit=None,\n",
    "                       iteration_limit=None,\n",
    "                       print_cost_while_training=False):\n",
    "        self.__print_start_training_message_and_log_time()\n",
    "        \n",
    "        self._setup_training()\n",
    "        self.__reset_thetas()\n",
    "        \n",
    "        if not training_algorithm or training_algorithm == RegressionAlgorithm.unspecified:\n",
    "            training_algorithm = self._calculate_optimized_training_alg()\n",
    "        \n",
    "        self._train(training_algorithm,\n",
    "                    learning_rate,\n",
    "                    time_limit,\n",
    "                    iteration_limit,\n",
    "                    print_cost_while_training)\n",
    "        \n",
    "        self.__print_end_training_message()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _setup_training(self):\n",
    "        print('Initializing trainer......')    \n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _calculate_optimized_training_alg(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _hypothesis(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _train(self,\n",
    "               training_algorithm=RegressionAlgorithm.unspecified,\n",
    "               learning_rate=0.01,\n",
    "               time_limit=None,\n",
    "               iteration_limit=None,\n",
    "               print_cost_while_training=False):\n",
    "        pass\n",
    "\n",
    "    def _cost(self):\n",
    "        h_theta_x = self._hypothesis()\n",
    "        diff = self._y - h_theta_x\n",
    "        diff_squared = np.power(diff, 2)\n",
    "        diff_squared_sum = np.sum(diff_squared.transpose(), axis=0)\n",
    "        theta_squared = np.power(self._theta, 2)\n",
    "        theta_squared_sum = np.sum(theta_squared.transpose(), axis=0)\n",
    "        total = diff_squared_sum + self._regularization_lambda * theta_squared_sum\n",
    "        return total / (2 * self._get_num_samples())\n",
    "    \n",
    "    def _derivative_of_cost(self):\n",
    "        h_theta_x = self._hypothesis().astype(np.float64)\n",
    "        diff = (h_theta_x - self._y).astype(np.float64)\n",
    "        diff_scaled_with_x_sum = (self._x.transpose() @ diff.transpose()).transpose()\n",
    "        regularization_vector = self._theta * self._regularization_lambda / self._get_num_samples()\n",
    "        return diff_scaled_with_x_sum / self._get_num_samples() + regularization_vector\n",
    "    \n",
    "    def _train_with_gradient_descent(self,\n",
    "                                      learning_rate=0.01,\n",
    "                                      time_limit=None,\n",
    "                                      iteration_limit=None,\n",
    "                                      print_cost_while_training=False):\n",
    "        last_cost = self._cost()\n",
    "        cost_not_change_count = 0\n",
    "        cost_check_frequency = 1000\n",
    "        i = 1\n",
    "        start_time = time.time()\n",
    "        condition = True\n",
    "        # If the cost hasn't changed in 20 (2 * 10) iterations, it converged.\n",
    "        while condition:\n",
    "            self._theta -= self._derivative_of_cost() * learning_rate\n",
    "            # Check and print cost every 10 iterations\n",
    "            if i == 1 or i % cost_check_frequency == 0:\n",
    "                current_cost = self._cost()\n",
    "                if print_cost_while_training:\n",
    "                    print('Cost of iteration {0}: {1}'.format(i, current_cost))\n",
    "                cost_equal = False\n",
    "                try:\n",
    "                    cost_equal = all(current_cost == last_cost)\n",
    "                except TypeError as e:\n",
    "                    cost_equal = current_cost == last_cost\n",
    "                if cost_equal:\n",
    "                    cost_not_change_count += 1\n",
    "                last_cost = current_cost\n",
    "            i += 1\n",
    "            condition = cost_not_change_count < 2\n",
    "            if time_limit is not None:\n",
    "                condition = condition and (time.time() - start_time) < time_limit\n",
    "            if iteration_limit is not None:\n",
    "                condition = condition and i <= iteration_limit\n",
    "\n",
    "    def _get_num_features(self):\n",
    "        return np.size(self._x, axis=1)\n",
    "    \n",
    "    def _get_num_samples(self):\n",
    "        return np.size(self._x, axis=0)\n",
    "    \n",
    "    def __print_start_training_message_and_log_time(self):\n",
    "        print('Started training......')\n",
    "        self.__training_start_time = time.time()\n",
    "    \n",
    "    def __print_end_training_message(self):\n",
    "        end_time = time.time()\n",
    "        print('Used {0:.10f} seconds to train model with {1} samples and {2} features.'.format\\\n",
    "             (end_time - self.__training_start_time, self._get_num_samples(), self._get_num_features() - 1))\n",
    "    \n",
    "    def __reset_thetas(self):\n",
    "        self._theta.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainerLinear(RegressionTrainer):\n",
    "    \n",
    "    def __get_feature_count_threshold(self):\n",
    "        return 10000\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _setup_training(self):\n",
    "        super()._setup_training()\n",
    "        self._theta = np.zeros(np.size(self._x, axis=1))\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _calculate_optimized_training_alg(self):\n",
    "        feature_count_small = self._get_num_features() < self.__get_feature_count_threshold()\n",
    "        if feature_count_small:\n",
    "            return RegressionAlgorithm.normal_equation\n",
    "        else:\n",
    "            return RegressionAlgorithm.gradient_descent\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _hypothesis(self):\n",
    "        return self._theta @ self._x.transpose()\n",
    "\n",
    "    # Override (abstract)\n",
    "    def _train(self,\n",
    "               training_algorithm=RegressionAlgorithm.unspecified,\n",
    "               learning_rate=0.01,\n",
    "               time_limit=None,\n",
    "               iteration_limit=None,\n",
    "               print_cost_while_training=False):\n",
    "        if training_algorithm == RegressionAlgorithm.gradient_descent:\n",
    "            self._train_with_gradient_descent(learning_rate, time_limit, iteration_limit, print_cost_while_training)\n",
    "        elif training_algorithm == RegressionAlgorithm.normal_equation:\n",
    "            self.__train_with_normal_equation()\n",
    "        else:\n",
    "            raise ValueError('Cannot start training, no linear regression algorithm specified.')\n",
    "\n",
    "    def __train_with_normal_equation(self):\n",
    "        x = self._x\n",
    "        x_trans = x.transpose()\n",
    "        y = self._y\n",
    "        regularization_matrix = np.identity(self._get_num_features())\n",
    "        regularization_matrix[0][0] = 0\n",
    "        regularization_matrix *= self._regularization_lambda\n",
    "        try:\n",
    "            result = np.linalg.inv(x_trans @ x + regularization_matrix) @ x_trans @ y\n",
    "        except ValueError as e:\n",
    "            raise Exception('Cannot calculate weights with normal equation.') from e\n",
    "        else:\n",
    "            self._theta = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainerLogistic(RegressionTrainer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 coefficient_matrix,\n",
    "                 outputs,\n",
    "                 regularization_lambda=0.0,\n",
    "                 output_case_sensitive=True):\n",
    "        super().__init__(coefficient_matrix, outputs, regularization_lambda)\n",
    "        self.__output_case_sensitive = output_case_sensitive\n",
    "    \n",
    "    @property\n",
    "    def categories(self):\n",
    "        return self.__categories\n",
    "\n",
    "    def __get_num_categories(self):\n",
    "        return np.size(self.__categories)\n",
    "    \n",
    "    # Override\n",
    "    def _setup_training(self):\n",
    "        super()._setup_training()\n",
    "        unique_cat, b_output = DataProcessor.get_unique_categories_and_binary_outputs(self._y, self.__output_case_sensitive)\n",
    "        self.__categories = unique_cat\n",
    "        self._y = b_output\n",
    "        cat_count = self.__get_num_categories()\n",
    "        if cat_count < 2:\n",
    "            raise ValueError('Cannot do logistic regression, there is only one kind of output.')\n",
    "        elif cat_count == 2:\n",
    "            self.__binary_classification = True\n",
    "            self._theta = np.zeros(self._get_num_features())\n",
    "        else:\n",
    "            self.__binary_classification = False\n",
    "            theta_shape = (cat_count, self._get_num_features())\n",
    "            self._theta = np.zeros(shape=theta_shape)\n",
    "            \n",
    "    # Override (abstract)\n",
    "    def _calculate_optimized_training_alg(self):\n",
    "        return RegressionAlgorithm.gradient_descent\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _hypothesis(self):\n",
    "        theta_transpose_x = self._theta @ self._x.transpose()\n",
    "        result = np.zeros(shape=(self.__get_num_categories(), self._get_num_samples()))\n",
    "        result.fill(math.e)\n",
    "        result = result ** (-1 * theta_transpose_x)\n",
    "        result = 1/ (1 + result)\n",
    "        return result\n",
    "\n",
    "    # Override (abstract)\n",
    "    def _train(self,\n",
    "               training_algorithm=RegressionAlgorithm.unspecified,\n",
    "               learning_rate=0.01,\n",
    "               time_limit=None,\n",
    "               iteration_limit=None,\n",
    "               print_cost_while_training=False):\n",
    "        if training_algorithm == RegressionAlgorithm.gradient_descent:\n",
    "            self._train_with_gradient_descent(learning_rate, time_limit, iteration_limit, print_cost_while_training)\n",
    "        else:\n",
    "            raise ValueError('Cannot start training, no logistic regression algorithm specified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionPredictor(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, weights, data_scalar):\n",
    "        self._weights = weights\n",
    "        self._data_scalar = data_scalar\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def predict(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionPredictorLinear(RegressionPredictor):\n",
    "    # Override (abstract)\n",
    "    def predict(self, data):\n",
    "        scaled_data = self._data_scalar.scale_new_data(data, False)\n",
    "        scaled_data = DataProcessor.add_x0_column(scaled_data)\n",
    "        return np.array(self._weights @ scaled_data.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionPredictorLogistic(RegressionPredictor):\n",
    "    \n",
    "    # Override\n",
    "    def __init__(self, weights, data_scalar, categories):\n",
    "        super().__init__(weights, data_scalar)\n",
    "        self.__categories = categories\n",
    "\n",
    "    # Override (abstract)\n",
    "    def predict(self, data):\n",
    "        scaled_data = self._data_scalar.scale_new_data(data, False)\n",
    "        scaled_data = (DataProcessor.add_x0_column(scaled_data)).astype(np.float64)\n",
    "        hypothesis = (self._weights).astype(np.float64) @ scaled_data.transpose()\n",
    "        max_ind = np.argmax(hypothesis, axis=0)\n",
    "        return np.array([self.__categories[ind] for ind in max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionSetup(metaclass=abc.ABCMeta):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 test_sample_ratio=0.05,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_lambda=0.0,\n",
    "                 regression_algorithm=RegressionAlgorithm.unspecified):\n",
    "        if data is None:\n",
    "            raise ValueError('Cannot initialize regression setup, no data.')\n",
    "        if not 0 <= test_sample_ratio < 1:\n",
    "            raise ValueError('Cannot initialize regression setup, invaild test sample ratio.')\n",
    "\n",
    "        self.data = data\n",
    "        self.test_sample_ratio = test_sample_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_lambda = regularization_lambda\n",
    "        self.regression_algorithm = regression_algorithm\n",
    "        \n",
    "    @abc.abstractproperty\n",
    "    def regression_type(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionSetupLinear(RegressionSetup):\n",
    "        \n",
    "    # Override (abstract)\n",
    "    @property\n",
    "    def regression_type(self):\n",
    "        return RegressionType.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionSetupLogistic(RegressionSetup):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 test_sample_ratio=0.05,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_lambda=0.0,\n",
    "                 regression_algorithm=RegressionAlgorithm.unspecified,\n",
    "                 output_case_sensitive=True):\n",
    "        super().__init__(data, test_sample_ratio, learning_rate, regularization_lambda, regression_algorithm)\n",
    "        self.output_case_sensitive = output_case_sensitive\n",
    "\n",
    "    # Override (abstract)\n",
    "    @property\n",
    "    def regression_type(self):\n",
    "        return RegressionType.logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Regression():\n",
    "    def __init__(self, setup):\n",
    "        self.__trained = False\n",
    "        self.__raw_data = setup.data\n",
    "        self.__reg_type = setup.regression_type\n",
    "        self.__test_sample_ratio = setup.test_sample_ratio\n",
    "        self.__learning_rate = setup.learning_rate\n",
    "        self.__regularization_lambda = setup.regularization_lambda\n",
    "        if self.__reg_type == RegressionType.logistic:\n",
    "            self.__output_case_sensitive = setup.output_case_sensitive\n",
    "        self.__reg_alg = setup.regression_algorithm\n",
    "        self.__setup_samples()\n",
    "        \n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            self.__trainer = RegressionTrainerLinear(coefficient_matrix=self.__x_training,\n",
    "                                                     outputs=self.__y_training,\n",
    "                                                     regularization_lambda=self.__regularization_lambda)\n",
    "            print('Linear trainer setup.')\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            self.__trainer = RegressionTrainerLogistic(coefficient_matrix=self.__x_training,\n",
    "                                                       outputs=self.__y_training,\n",
    "                                                       regularization_lambda=self.__regularization_lambda,\n",
    "                                                       output_case_sensitive=self.__output_case_sensitive)\n",
    "            print('Logistic trainer setup.')\n",
    "            \n",
    "    def train(self,\n",
    "              time_limit=None,\n",
    "              iteration_limit=None,\n",
    "              print_cost=False):\n",
    "        self.__trained = False\n",
    "        if not self.__trainer:\n",
    "            raise AttributeError('Cannot start training, trainer not found.')\n",
    "        self.__trainer.start_training(training_algorithm=self.__reg_alg,\n",
    "                                      learning_rate=self.__learning_rate,\n",
    "                                      time_limit=time_limit,\n",
    "                                      iteration_limit=iteration_limit,\n",
    "                                      print_cost_while_training=print_cost)\n",
    "        self.__trained = True\n",
    "        self.__setup_predictor()\n",
    "        self.__print_error_rate()\n",
    "\n",
    "    def predict(self, data):\n",
    "        if not self.__predictor:\n",
    "            raise Exception('Cannot predict, no predictor found.')\n",
    "        return (self.__predictor.predict(data)).flatten()\n",
    "\n",
    "    def __setup_samples(self):\n",
    "        if self.__raw_data is None:\n",
    "            raise ValueError('Cannot setup samples, no data.')\n",
    "        num_training_sample = self.__get_training_sample_count()\n",
    "        self.__x_training = self.__raw_data[:num_training_sample, :-1]\n",
    "        self.__y_training = self.__raw_data[:num_training_sample, -1]\n",
    "        self.__x_testing = self.__raw_data[num_training_sample:, :-1]\n",
    "        self.__y_testing = self.__raw_data[num_training_sample:, -1]\n",
    "        self.__preprocess_training_set_features()\n",
    "    \n",
    "    def __setup_predictor(self):\n",
    "        if not self.__trained:\n",
    "            raise Exception('Cannot setup predictor, model has not been trained.')\n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            self.__predictor = RegressionPredictorLinear(weights=self.__trainer.weights,\n",
    "                                                         data_scalar=self.__data_scalar)\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            self.__predictor = RegressionPredictorLogistic(weights=self.__trainer.weights,\n",
    "                                                           data_scalar=self.__data_scalar,\n",
    "                                                           categories=self.__trainer.categories)\n",
    "\n",
    "    def __print_error_rate(self):\n",
    "        error_rate = self.__get_testing_set_error_rate()\n",
    "        print('Error rate is {0:.2f}%.'.format(error_rate * 100))\n",
    "\n",
    "    def __get_training_sample_count(self):\n",
    "        total_sample_count = np.size(self.__raw_data, axis=0)\n",
    "        return math.ceil((1.0 - self.__test_sample_ratio) * total_sample_count)\n",
    "    \n",
    "    def __preprocess_training_set_features(self):\n",
    "        if self.__x_training is None:\n",
    "            raise ValueError('Cannot preprocess training set features, no data.')\n",
    "        self.__data_scalar = DataScalar(self.__x_training)\n",
    "        self.__x_training = self.__data_scalar.scaled_data()\n",
    "        self.__x_training = DataProcessor.add_x0_column(self.__x_training)\n",
    "    \n",
    "    def __get_testing_set_error_rate(self):\n",
    "        if not self.__predictor:\n",
    "            raise Exception('Cannot get error rate, no predictor found.')\n",
    "        testing_sample_predictions = self.predict(self.__x_testing)\n",
    "        return self.__get_error_rate(testing_sample_predictions, self.__y_testing)\n",
    "            \n",
    "    def __get_error_rate(self, prediction, actual):\n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            diff = np.abs((prediction - actual)/actual)\n",
    "            diff = diff[~np.isnan(diff)]\n",
    "            return np.average(diff)\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            match = prediction == actual\n",
    "            match_count = np.count_nonzero(match)\n",
    "            total_count = np.size(actual)\n",
    "            return (total_count - match_count) / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully queried data.\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "df = pd.read_csv('housing_data/housing.data', header=None, delim_whitespace=True)\n",
    "data_linear_reg = df.as_matrix()\n",
    "if data_linear_reg is not None:\n",
    "    print('Successfully queried data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLinear(data=data_linear_reg,\n",
    "                              test_sample_ratio=0.05,\n",
    "                              regularization_lambda=150,\n",
    "                              regression_algorithm=RegressionAlgorithm.normal_equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear trainer setup.\n",
      "Started training......\n",
      "Initializing trainer......\n",
      "Used 0.0004699230 seconds to train model with 481 samples and 13 features.\n",
      "Error rate is 15.32%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ 16.8].\n",
      "Actual: 13.6.\n"
     ]
    }
   ],
   "source": [
    "features = np.matrix('0.10574   0.00  27.740  0  0.6090  5.9830  98.80  1.8681   4  711.0  20.10 390.11  18.07')\n",
    "actual = 13.6\n",
    "prediction = regression.predict(features)\n",
    "prediction = np.round(prediction, 2)\n",
    "print('Prediction: {0}.\\nActual: {1}.'.format(prediction, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLinear(data=data_linear_reg,\n",
    "                              test_sample_ratio=0.05,\n",
    "                              learning_rate=0.01,\n",
    "                              regularization_lambda=30,\n",
    "                              regression_algorithm=RegressionAlgorithm.gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear trainer setup.\n",
      "Started training......\n",
      "Initializing trainer......\n",
      "Used 0.3756949902 seconds to train model with 481 samples and 13 features.\n",
      "Error rate is 15.93%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train(time_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ 14.08373555], actual: 13.6.\n"
     ]
    }
   ],
   "source": [
    "features = np.matrix('0.10574   0.00  27.740  0  0.6090  5.9830  98.80  1.8681   4  711.0  20.10 390.11  18.07')\n",
    "actual = 13.6\n",
    "prediction = regression.predict(features)\n",
    "print('Prediction: {0}, actual: {1}.'.format(prediction, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully queried data.\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "df = pd.read_csv('iris_data/iris.data', header=None)\n",
    "data_logistic_reg = df.as_matrix()\n",
    "if data_logistic_reg is not None:\n",
    "    print('Successfully queried data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLogistic(data=data_logistic_reg,\n",
    "                                test_sample_ratio=0.05,\n",
    "                                learning_rate=1,\n",
    "                                regularization_lambda=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic trainer setup.\n",
      "Started training......\n",
      "Initializing trainer......\n",
      "Used 0.8183560371 seconds to train model with 143 samples and 4 features.\n",
      "Error rate is 0.00%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train(time_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['Iris-virginica'], actual: Iris-virginica.\n"
     ]
    }
   ],
   "source": [
    "features = np.matrix('6.4 2.8 5.6 2.2')\n",
    "actual = 'Iris-virginica'\n",
    "prediction = regression.predict(features)\n",
    "print('Prediction: {0}, actual: {1}.'.format(prediction, actual))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
