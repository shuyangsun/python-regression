{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This Python script is to do linear or logistic regression, using gradient descent or normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "# Scientific Calculation Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Built-in Module Imports\n",
    "import math\n",
    "import time\n",
    "import enum\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression Type\n",
    "class RegressionType(enum.Enum):\n",
    "    linear = 1\n",
    "    logistic = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Algorithm Class\n",
    "class RegressionAlgorithm(enum.Enum):\n",
    "    unspecified = 0\n",
    "    gradient_descent = 1\n",
    "    normal_equation = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataProcessor():\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_x0_column(A):\n",
    "        return np.insert(A, obj=0, values=1, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def augmented_to_coefficient_and_b(A):\n",
    "        return (A[:, :-1], A[:, -1])\n",
    "\n",
    "    @staticmethod\n",
    "    def partition(A, atInd):\n",
    "        return (A[:atInd], A[atInd:])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_unique_categories(output, case_sensitive=True):\n",
    "        if not case_sensitive:\n",
    "            output = [x.lower() if isinstance(x, str) else x for x in output]\n",
    "        return np.unique(output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_unique_categories_and_binary_outputs(output, case_sensitive=True):\n",
    "        unique_cat = DataProcessor.get_unique_categories(output, case_sensitive)\n",
    "\n",
    "        if np.size(unique_cat) <= 2:\n",
    "            outputs_b = np.zeros(np.size(output))\n",
    "            mask_0 = (output != unique_cat[0])\n",
    "            mask_1 = (output == unique_cat[0])\n",
    "            outputs_b[mask_0] = 0\n",
    "            outputs_b[mask_1] = 1\n",
    "        else:\n",
    "            outputs_b = np.tile(output, (np.size(unique_cat), 1))\n",
    "            for i, cat in enumerate(unique_cat):\n",
    "                row = outputs_b[i]\n",
    "                mask_0 = (output != cat)\n",
    "                mask_1 = (output == cat)\n",
    "                row[mask_0] = 0\n",
    "                row[mask_1] = 1\n",
    "        \n",
    "        return (unique_cat, outputs_b)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataScalar():\n",
    "    \n",
    "    def __init__(self, data, data_has_x0_column=False):\n",
    "        self.__data = data\n",
    "        self.__data_has_x0_column = data_has_x0_column \n",
    "        self.__scalars = np.ones(np.size(data, axis=1))\n",
    "        self.__calculate_scalars()\n",
    "    \n",
    "    def scaled_data(self):\n",
    "        return self.scale_new_data(self.__data, self.__data_has_x0_column)\n",
    "    \n",
    "    def scale_new_data(self, data, input_has_x0_column=False):\n",
    "        if input_has_x0_column:\n",
    "            avg = np.insert(self.__avg, obj=0, values=0)\n",
    "            std = np.insert(self.__std, obj=0, values=1)\n",
    "        else:\n",
    "            avg = self.__avg\n",
    "            std = self.__std\n",
    "        return (data - avg) / std\n",
    "        \n",
    "    def __calculate_scalars(self):\n",
    "        if self.__data_has_x0_column:\n",
    "            self.__avg = np.average(self.__data[:, 1:], axis=0)\n",
    "            self.__std = np.std(self.__data[:, 1:], axis=0)\n",
    "        else:\n",
    "            self.__avg = np.average(self.__data, axis=0)\n",
    "            self.__std = np.std(self.__data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainer(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    def __init__(self, coefficient_matrix, outputs, regularization_lambda=0.0):\n",
    "        self.__x = coefficient_matrix\n",
    "        self.__y = outputs\n",
    "        self.__regularization_lambda = regularization_lambda\n",
    "        self.__setup_training()\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__theta\n",
    "\n",
    "    def start_training(self,\n",
    "                      training_algorithm=RegressionAlgorithm.unspecified,\n",
    "                      learning_rate=0.01,\n",
    "                      print_cost_while_training=False):\n",
    "        self.__reset_thetas()\n",
    "        self.__print_start_training_message_and_log_time()\n",
    "        \n",
    "        self.__setup_training()\n",
    "        if not training_algorithm:\n",
    "            training_algorithm = self.__calculate_optimized_training_alg()\n",
    "        \n",
    "        self.__train(self,\n",
    "                     training_algorithm,\n",
    "                     learning_rate,\n",
    "                     print_cost_while_training)\n",
    "        \n",
    "        self.__print_end_training_message()\n",
    "\n",
    "    # Should override in subclass\n",
    "    def __setup_training(self):\n",
    "        print('Initializing trainer......')    \n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __calculate_optimized_training_alg(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __hypothesis(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __cost(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __derivative_of_cost(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __train(self,\n",
    "                training_algorithm=RegressionAlgorithm.unspecified,\n",
    "                learning_rate=0.01,\n",
    "                print_cost_while_training=False):\n",
    "        pass\n",
    "    \n",
    "    def __get_num_features(self):\n",
    "        return np.size(self.__x, axis=1)\n",
    "    \n",
    "    def __get_num_samples(self):\n",
    "        return np.size(self.__x, axis=0)\n",
    "    \n",
    "    def __print_start_training_message_and_log_time(self):\n",
    "        print('Started training......')\n",
    "        self.__training_start_time = time.time()\n",
    "    \n",
    "    def __print_end_training_message(self):\n",
    "        end_time = time.time()\n",
    "        print('Used {0:.10f} seconds to train model with {1} samples and {2} features.'.format\\\n",
    "             (end_time - start_time, self.__get_num_samples, self.__get_num_features - 1))\n",
    "    \n",
    "    def __reset_thetas(self):\n",
    "        self.__theta.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainerLinear(RegressionTrainer):\n",
    "    \n",
    "    def __get_feature_count_threshold(self):\n",
    "        return 10000\n",
    "    \n",
    "    # Override\n",
    "    def __setup_training(self):\n",
    "        super().__setup_training()\n",
    "        self.__theta = np.zeros(np.size(self.__data, axis=1))\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def __calculate_optimized_training_alg(self):\n",
    "        feature_count_small = self.__get_num_features < self.__get_feature_count_threshold()\n",
    "        if feature_count_small:\n",
    "            return RegressionAlgorithm.normal_equation\n",
    "        else:\n",
    "            return RegressionAlgorithm.gradient_descent\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def __hypothesis(self):\n",
    "        return self.__theta @ self.__x.transpose()\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def __cost(self):\n",
    "        h_theta_x = self.__hypothesis()\n",
    "        diff = self.__y - h_theta_x\n",
    "        diff_squared = np.power(diff, 2)\n",
    "        diff_squared_sum = np.sum(diff_squared)\n",
    "        theta_squared = np.power(self.__theta, 2)\n",
    "        theta_squared_sum = np.sum(theta_squared)\n",
    "        total = diff_squared_sum + self.__regularization_lambda * theta_squared_sum\n",
    "        return total / (2 * self.__get_num_samples())\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def __derivative_of_cost(self):\n",
    "        h_theta_x = self.__hypothesis()\n",
    "        diff = h_theta_x - self.__y\n",
    "        diff_scaled_with_x = (self.__x.transpose() * diff).transpose()\n",
    "        regularization_vector = self.__theta * self.__regularization_lambda / self.__get_num_samples()\n",
    "        return np.average(diff_scaled_with_x, axis=0) + regularization_vector\n",
    "\n",
    "    # Override (abstract)\n",
    "    def __train(self,\n",
    "                training_algorithm=RegressionAlgorithm.unspecified,\n",
    "                learning_rate=0.01,\n",
    "                print_cost_while_training=False):\n",
    "        if training_algorithm == RegressionAlgorithm.gradient_descent:\n",
    "            self.__train_with_gradient_descent(learning_rate, print_cost_while_training)\n",
    "        elif training_algorithm == RegressionAlgorithm.normal_equation:\n",
    "            self.__train_with_normal_equation()\n",
    "        else:\n",
    "            raise ValueError('Cannot start training, no linear regression algorithm specified.')\n",
    "\n",
    "    def __train_with_gradient_descent(self,\n",
    "                                      learning_rate=0.01,\n",
    "                                      print_cost_while_training=False):\n",
    "        last_cost = self.__cost_of_training_set()\n",
    "        cost_not_change_count = 0\n",
    "        cost_check_frequency = 10\n",
    "        i = 1\n",
    "        # If the cost hasn't changed in 20 (2 * 10) iterations, it converged.\n",
    "        while cost_not_change_count < 2:\n",
    "            self.__theta -= self.__derivative_of_cost() * self.__learning_rate\n",
    "            # Check and print cost every 10 iterations\n",
    "            if i == 1 or i % cost_check_frequency == 0:\n",
    "                current_cost = self.__cost()\n",
    "                if print_cost_while_training:\n",
    "                    print('Cost of iteration {0}: {1:.2f}'.format(i, current_cost))\n",
    "                if current_cost == last_cost:\n",
    "                    cost_not_change_count += 1\n",
    "                last_cost = current_cost\n",
    "            i += 1\n",
    "\n",
    "    def __train_with_normal_equation(self):\n",
    "        x = self.__x\n",
    "        x_trans = x.transpose()\n",
    "        y = self.__y\n",
    "        regularization_matrix = np.identity(self.__get_num_features())\n",
    "        regularization_matrix[0][0] = 0\n",
    "        regularization_matrix *= self.__regularization_lambda\n",
    "        try:\n",
    "            result = np.linalg.inv(x_trans @ x + regularization_matrix) @ x_trans @ y\n",
    "        except ValueError as e:\n",
    "            raise Exception('Cannot calculate weights with normal equation.') from e\n",
    "        else:\n",
    "            self.__theta = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainerLogistic(RegressionTrainer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 coefficient_matrix,\n",
    "                 outputs,\n",
    "                 regularization_lambda=0.0,\n",
    "                 output_case_sensitive=True):\n",
    "        super().__init__(coefficient_matrix, outputs, regularization_lambda)\n",
    "        self.__output_case_sensitive = output_case_sensitive\n",
    "    \n",
    "    @property\n",
    "    def categories(self):\n",
    "        return self.__categories\n",
    "\n",
    "    def __get_num_categories(self):\n",
    "        if self.__regression_type == RegressionType.logistic:\n",
    "            return np.size(self.__categories)\n",
    "        else:\n",
    "            raise InterruptedError('Should not query number of categories in non-logistic regression.')\n",
    "    \n",
    "    # Override\n",
    "    def __setup_training(self):\n",
    "        super().__setup_training()\n",
    "        unique_cat, b_output = DataProcessor.get_unique_categories_and_binary_outputs(self.__y, self.__output_case_sensitive)\n",
    "        self.__categories = unique_cat\n",
    "        self.__y = b_output\n",
    "        feature_count = np.size(self.__data, axis=1)\n",
    "        cat_count = np.size(self.__categories, axis=0)\n",
    "        if cat_count < 2:\n",
    "            raise ValueError('Cannot do logistic regression, there is only one kind of output.')\n",
    "        elif cat_count == 2:\n",
    "            self.__binary_classification = True\n",
    "            self.__theta = np.zeros(feature_count)\n",
    "        else:\n",
    "            self.__binary_classification = False\n",
    "            theta_shape = (cat_count, feature_count)\n",
    "            self.__theta = np.zeros(shape=theta_shape)\n",
    "            \n",
    "    # Override (abstract)\n",
    "    def __calculate_optimized_training_alg(self):\n",
    "        return RegressionAlgorithm.gradient_descent\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def __hypothesis(self):\n",
    "        theta_transpose_x = self.__theta @ self.__x.transpose()\n",
    "        result = np.zeros(shape=(self.__get_num_categories, self.__get_num_samples))\n",
    "        result.fill(math.e)\n",
    "        result = result ** (-1 * theta_transpose_x)\n",
    "        result = 1/ (1 + result)\n",
    "        return result\n",
    "\n",
    "    # Override (abstract)\n",
    "    def __train(self,\n",
    "                training_algorithm=RegressionAlgorithm.unspecified,\n",
    "                learning_rate=0.01,\n",
    "                print_cost_while_training=False):\n",
    "        if training_algorithm == RegressionAlgorithm.gradient_descent:\n",
    "            self.__train_with_gradient_descent(learning_rate, print_cost_while_training)\n",
    "        else:\n",
    "            raise ValueError('Cannot start training, no logistic regression algorithm specified.')\n",
    "    \n",
    "    def __train_with_gradient_descent(self,\n",
    "                                      learning_rate=0.01,\n",
    "                                      print_cost_while_training=False):\n",
    "        pass # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionPredictor(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, weights, data_scalar):\n",
    "        self.__weights = weights\n",
    "        self.__data_scalar = data_scalar\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def predict(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionPredictorLinear(RegressionPredictor):\n",
    "    # Override (abstract)\n",
    "    def predict(self, data):\n",
    "        scaled_data = self.__data_scalar.scale_new_data(data, False)\n",
    "        scaled_data = DataProcessor.add_x0_column(scaled_data)\n",
    "        return self.__weights @ scaled_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionPredictorLogistic(RegressionPredictor):\n",
    "    \n",
    "    # Override\n",
    "    def __init__(self, weights, data_scalar, categories):\n",
    "        super().__init__(weights, data_scalar)\n",
    "        self.__categories = categories\n",
    "\n",
    "    # Override (abstract)\n",
    "    def predict(self, data):\n",
    "        scaled_data = self.__data_scalar.scale_new_data(data, False)\n",
    "        scaled_data = DataProcessor.add_x0_column(scaled_data)\n",
    "        hypothesis = self.__weights @ scaled_data.transpose()\n",
    "        max_ind = np.argmax(hypothesis, axis=0)\n",
    "        return [self.__categories[ind] for ind in max_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionSetup(metaclass=abc.ABCMeta):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 test_sample_ratio=0.05,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_lambda=0.0,\n",
    "                 regression_algorithm=RegressionAlgorithm.unspecified):\n",
    "        if data is None:\n",
    "            raise ValueError('Cannot initialize regression setup, no data.')\n",
    "        if not 0 <= test_sample_ratio < 1:\n",
    "            raise ValueError('Cannot initialize regression setup, invaild test sample ratio.')\n",
    "\n",
    "        self.data = data\n",
    "        self.test_sample_ratio = test_sample_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_lambda = regularization_lambda\n",
    "        self.regression_algorithm = regression_algorithm\n",
    "        \n",
    "    @abc.abstractproperty\n",
    "    def regression_type(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionSetupLinear(RegressionSetup):\n",
    "        \n",
    "    # Override (abstract)\n",
    "    @property\n",
    "    def regression_type(self):\n",
    "        return RegressionType.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionSetupLogistic(RegressionSetup):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 test_sample_ratio=0.05,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_lambda=0.0,\n",
    "                 regression_algorithm=RegressionAlgorithm.unspecified,\n",
    "                 output_case_sensitive=True):\n",
    "        super().__init__(data, test_sample_ratio, learning_rate, regularization_lambda, regression_algorithm)\n",
    "        self.output_case_sensitive = output_case_sensitive\n",
    "\n",
    "    # Override (abstract)\n",
    "    @property\n",
    "    def regression_type(self):\n",
    "        return RegressionType.logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Regression():\n",
    "    def __init__(self, setup):\n",
    "        self.__trained = False\n",
    "        self.__raw_data = setup.data\n",
    "        self.__reg_type = setup.regression_type\n",
    "        self.__test_sample_ratio = setup.test_sample_ratio\n",
    "        self.__learning_rate = setup.learning_rate\n",
    "        self.__regularization_lambda = setup.regularization_lambda\n",
    "        if self.__reg_type == RegressionType.logistic:\n",
    "            self.__output_case_sensitive = setup.output_case_sensitive\n",
    "        elif self.__reg_type == RegressionType.linear:\n",
    "            self.__reg_alg = setup.regression_algorithm\n",
    "        self.__setup_samples()\n",
    "        \n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            self.__trainer = RegressionTrainerLinear(coefficient_matrix=self.__x_training,\n",
    "                                                     outputs=self.__y_training,\n",
    "                                                     regularization_lambda=self.__regularization_lambda)\n",
    "            print('Linear trainer setup.')\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            self.__trainer = RegressionTrainerLogistic(coefficient_matrix=self.__x_training,\n",
    "                                                       outputs=self.__y_training,\n",
    "                                                       regularization_lambda=self.__regularization_lambda,\n",
    "                                                       output_case_sensitive=self.__output_case_sensitive)\n",
    "            print('Logistic trainer setup.')\n",
    "            \n",
    "    def train(self):\n",
    "        self.__trained = False\n",
    "        if not self.__trainer:\n",
    "            raise AttributeError('Cannot start training, trainer not found.')\n",
    "        self.__trainer.start_training(training_algorithm=self.__reg_alg,\n",
    "                                      learning_rate=self.__learning_rate,\n",
    "                                      print_cost_while_training=False)\n",
    "        self.__trained = True\n",
    "        self.__setup_predictor()\n",
    "        self.__print_error_rate()\n",
    "\n",
    "    def predict(self, data):\n",
    "        if not self.__predictor:\n",
    "            raise Exception('Cannot predict, no predictor found.')\n",
    "        return self.__predictor.predict(data)\n",
    "\n",
    "    def __setup_samples(self):\n",
    "        if self.__raw_data is None:\n",
    "            raise ValueError('Cannot setup samples, no data.')\n",
    "        num_training_sample = self.__get_training_sample_count()\n",
    "        self.__x_training = self.__raw_data[:num_training_sample, :-1]\n",
    "        self.__y_training = self.__raw_data[:num_training_sample, -1]\n",
    "        self.__x_testing = self.__raw_data[num_training_sample:, :-1]\n",
    "        self.__y_testing = self.__raw_data[num_training_sample:, -1]\n",
    "        self.__preprocess_training_set_features()\n",
    "    \n",
    "    def __setup_predictor(self):\n",
    "        if not self.__trained:\n",
    "            raise Exception('Cannot setup predictor, model has not been trained.')\n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            self.__predictor = RegressionPredictorLinear(weights=self.__trainer.weights,\n",
    "                                                         data_scalar=self.__data_scalar)\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            self.__predictor = RegressionPredictorLogistic(weights=self.__trainer.weights,\n",
    "                                                           data_scalar=self.__data_scalar,\n",
    "                                                           categories=self.__trainer.categories)\n",
    "\n",
    "    def __print_error_rate(self):\n",
    "        error_rate = self.__get_testing_set_error_rate()\n",
    "        print('Error rate is {0:.2f}%.'.format(error_rate * 100))\n",
    "\n",
    "    def __get_training_sample_count(self):\n",
    "        total_sample_count = np.size(self.__raw_data, axis=0)\n",
    "        return math.ceil((1.0 - self.__test_sample_ratio) * total_sample_count)\n",
    "    \n",
    "    def __preprocess_training_set_features(self):\n",
    "        if self.__x_training is None:\n",
    "            raise ValueError('Cannot preprocess training set features, no data.')\n",
    "        self.__data_scalar = DataScalar(self.__x_training)\n",
    "        self.__x_training = self.__data_scalar.scaled_data()\n",
    "        self.__x_training = DataProcessor.add_x0_column(self.__x_training)\n",
    "    \n",
    "    def __get_testing_set_error_rate(self):\n",
    "        if not self.__predictor:\n",
    "            raise Exception('Cannot get error rate, no predictor found.')\n",
    "        testing_sample_predictions = self.predict(self.__x_testing)\n",
    "        return self.__get_error_rate(testing_sample_predictions, self.__y_testing)\n",
    "            \n",
    "    def __get_error_rate(self, prediction, actual):\n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            diff = np.abs((prediction - actual)/actual)\n",
    "            diff = diff[~np.isnan(diff)]\n",
    "            return np.average(diff)\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            match = prediction == actual\n",
    "            match_count = np.count_nonzero(match)\n",
    "            total_count = np.size(actual)\n",
    "            return (total_count - match_count) / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully queried data.\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "df = pd.read_csv('housing_data/housing.data', header=None, delim_whitespace=True)\n",
    "data_linear_reg = df.as_matrix()\n",
    "if data_linear_reg is not None:\n",
    "    print('Successfully queried data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLinear(data=data_linear_reg,\n",
    "                              test_sample_ratio=0.05,\n",
    "                              learning_rate=0.01,\n",
    "                              regularization_lambda=0.0,\n",
    "                              regression_algorithm=RegressionAlgorithm.gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class RegressionTrainerLinear with abstract methods _RegressionTrainer__calculate_optimized_training_alg, _RegressionTrainer__cost, _RegressionTrainer__derivative_of_cost, _RegressionTrainer__hypothesis, _RegressionTrainer__train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-387-ac2784f4f242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-384-65d84a5e0425>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, setup)\u001b[0m\n\u001b[1;32m     16\u001b[0m             self.__trainer = RegressionTrainerLinear(coefficient_matrix=self.__x_training,\n\u001b[1;32m     17\u001b[0m                                                      \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__y_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                      regularization_lambda=self.__regularization_lambda)\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Linear trainer setup.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reg_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRegressionType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class RegressionTrainerLinear with abstract methods _RegressionTrainer__calculate_optimized_training_alg, _RegressionTrainer__cost, _RegressionTrainer__derivative_of_cost, _RegressionTrainer__hypothesis, _RegressionTrainer__train"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
