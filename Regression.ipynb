{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This Python script is to do linear or logistic regression, using gradient descent or normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "# Scientific Calculation Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "# Built-in Module Imports\n",
    "import math\n",
    "import time\n",
    "import enum\n",
    "import abc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression Type\n",
    "class RegressionType(enum.Enum):\n",
    "    linear = 1\n",
    "    logistic = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Algorithm Class\n",
    "class RegressionAlgorithm(enum.Enum):\n",
    "    unspecified = 0\n",
    "    gradient_descent = 1\n",
    "    normal_equation = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataProcessor():\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_x0_column(A):\n",
    "        return np.insert(A, obj=0, values=1, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def augmented_to_coefficient_and_b(A):\n",
    "        return (A[:, :-1], A[:, -1])\n",
    "\n",
    "    @staticmethod\n",
    "    def partition(A, atInd):\n",
    "        return (A[:atInd], A[atInd:])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_unique_categories(output, case_sensitive=True):\n",
    "        if not case_sensitive:\n",
    "            output = [x.lower() if isinstance(x, str) else x for x in output]\n",
    "        return np.unique(output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_unique_categories_and_binary_outputs(output, case_sensitive=True):\n",
    "        unique_cat = DataProcessor.get_unique_categories(output, case_sensitive)\n",
    "\n",
    "        if np.size(unique_cat) <= 2:\n",
    "            outputs_b = np.zeros(np.size(output))\n",
    "            mask_0 = (output != unique_cat[0])\n",
    "            mask_1 = (output == unique_cat[0])\n",
    "            outputs_b[mask_0] = 0\n",
    "            outputs_b[mask_1] = 1\n",
    "        else:\n",
    "            outputs_b = np.tile(output, (np.size(unique_cat), 1))\n",
    "            for i, cat in enumerate(unique_cat):\n",
    "                row = outputs_b[i]\n",
    "                mask_0 = (output != cat)\n",
    "                mask_1 = (output == cat)\n",
    "                row[mask_0] = 0\n",
    "                row[mask_1] = 1\n",
    "        \n",
    "        return (unique_cat, outputs_b)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureNormalizer():\n",
    "    \n",
    "    def __init__(self, data, data_has_x0_column=False):\n",
    "        self.__data = data.astype(np.float64)\n",
    "        self.__data_has_x0_column = data_has_x0_column \n",
    "        self.__scalars = np.ones(np.size(data, axis=1))\n",
    "        self.__calculate_scalars()\n",
    "    \n",
    "    def normalized_feature(self):\n",
    "        return self.normalize_new_feature(self.__data, self.__data_has_x0_column)\n",
    "    \n",
    "    def normalize_new_feature(self, data, input_has_x0_column=False):\n",
    "        if input_has_x0_column:\n",
    "            avg = np.insert(self.__avg, obj=0, values=0)\n",
    "            std = np.insert(self.__std, obj=0, values=1)\n",
    "        else:\n",
    "            avg = self.__avg\n",
    "            std = self.__std\n",
    "        return (data - avg) / std\n",
    "        \n",
    "    def __calculate_scalars(self):\n",
    "        if self.__data_has_x0_column:\n",
    "            self.__avg = np.average(self.__data[:, 1:], axis=0)\n",
    "            self.__std = np.std(self.__data[:, 1:], axis=0)\n",
    "        else:\n",
    "            self.__avg = np.average(self.__data, axis=0)\n",
    "            self.__std = np.std(self.__data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainer(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    def __init__(self, coefficient_matrix, outputs, regularization_lambda=0.0):\n",
    "        self._x = coefficient_matrix\n",
    "        self._y = outputs\n",
    "        self._regularization_lambda = regularization_lambda\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self._theta\n",
    "\n",
    "    def start_training(self,\n",
    "                       training_algorithm=RegressionAlgorithm.unspecified,\n",
    "                       learning_rate=0.01,\n",
    "                       time_limit=None,\n",
    "                       iteration_limit=None,\n",
    "                       print_cost_while_training=False):\n",
    "        self.__print_start_training_message_and_log_time()\n",
    "        \n",
    "        self._setup_training()\n",
    "        self.__reset_thetas()\n",
    "        \n",
    "        if not training_algorithm or training_algorithm == RegressionAlgorithm.unspecified:\n",
    "            training_algorithm = self._calculate_optimized_training_alg()\n",
    "        \n",
    "        print('Training......')\n",
    "\n",
    "        self._train(training_algorithm,\n",
    "                    learning_rate,\n",
    "                    time_limit,\n",
    "                    iteration_limit,\n",
    "                    print_cost_while_training)\n",
    "        \n",
    "        self.__print_end_training_message()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _setup_training(self):\n",
    "        print('Setting up trainer......')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _calculate_optimized_training_alg(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _hypothesis(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _train(self,\n",
    "               training_algorithm=RegressionAlgorithm.unspecified,\n",
    "               learning_rate=0.01,\n",
    "               time_limit=None,\n",
    "               iteration_limit=None,\n",
    "               print_cost_while_training=False):\n",
    "        pass\n",
    "\n",
    "    def _cost(self):\n",
    "        h_theta_x = self._hypothesis()\n",
    "        diff = self._y - h_theta_x\n",
    "        diff_squared = np.power(diff, 2)\n",
    "        diff_squared_sum = np.sum(diff_squared.transpose(), axis=0)\n",
    "        theta_squared = np.power(self._theta, 2)\n",
    "        theta_squared = theta_squared.transpose()\n",
    "        theta_squared[0] = 0\n",
    "        theta_squared = theta_squared.transpose()\n",
    "        theta_squared_sum = np.sum(theta_squared.transpose(), axis=0)\n",
    "        total = diff_squared_sum + self._regularization_lambda * theta_squared_sum\n",
    "        return total / (2 * self._get_num_samples())\n",
    "    \n",
    "    def _derivative_of_cost(self):\n",
    "        h_theta_x = self._hypothesis().astype(np.float64)\n",
    "        diff = (h_theta_x - self._y).astype(np.float64)\n",
    "        diff_scaled_with_x_sum = (self._x.transpose() @ diff.transpose()).transpose()\n",
    "        regularization_vector = self._theta * self._regularization_lambda / self._get_num_samples()\n",
    "        regularization_vector = regularization_vector.transpose()\n",
    "        regularization_vector[0] = np.float64(0)\n",
    "        regularization_vector = regularization_vector.transpose()\n",
    "        return diff_scaled_with_x_sum / self._get_num_samples() + regularization_vector\n",
    "    \n",
    "    def _train_with_gradient_descent(self,\n",
    "                                      learning_rate=0.01,\n",
    "                                      time_limit=None,\n",
    "                                      iteration_limit=None,\n",
    "                                      print_cost_while_training=False):\n",
    "        last_cost = self._cost()\n",
    "        cost_not_change_count = 0\n",
    "        cost_check_frequency = 1000\n",
    "        i = 1\n",
    "        start_time = time.time()\n",
    "        condition = True\n",
    "        # If the cost hasn't changed in 20 (2 * 10) iterations, it converged.\n",
    "        while condition:\n",
    "            self._theta -= self._derivative_of_cost() * learning_rate\n",
    "            # Check and print cost every 10 iterations\n",
    "            if i == 1 or i % cost_check_frequency == 0:\n",
    "                current_cost = self._cost()\n",
    "                if print_cost_while_training:\n",
    "                    print('Cost of iteration {0}: {1}'.format(i, current_cost))\n",
    "                cost_equal = False\n",
    "                try:\n",
    "                    cost_equal = all(current_cost == last_cost)\n",
    "                except TypeError as e:\n",
    "                    cost_equal = current_cost == last_cost\n",
    "                if cost_equal:\n",
    "                    cost_not_change_count += 1\n",
    "                last_cost = current_cost\n",
    "            i += 1\n",
    "            condition = cost_not_change_count < 2\n",
    "            if time_limit is not None:\n",
    "                condition = condition and (time.time() - start_time) < time_limit\n",
    "            if iteration_limit is not None:\n",
    "                condition = condition and i <= iteration_limit\n",
    "\n",
    "    def _get_num_features(self):\n",
    "        return np.size(self._x, axis=1)\n",
    "    \n",
    "    def _get_num_samples(self):\n",
    "        return np.size(self._x, axis=0)\n",
    "    \n",
    "    def __print_start_training_message_and_log_time(self):\n",
    "        print('Initializing......')\n",
    "        self.__training_start_time = time.time()\n",
    "    \n",
    "    def __print_end_training_message(self):\n",
    "        end_time = time.time()\n",
    "        print('Used {0:.10f} seconds to train model with {1} samples and {2} features.'.format\\\n",
    "             (end_time - self.__training_start_time, self._get_num_samples(), self._get_num_features() - 1))\n",
    "        print('Training finished.')\n",
    "    \n",
    "    def __reset_thetas(self):\n",
    "        self._theta.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainerLinear(RegressionTrainer):\n",
    "    \n",
    "    def __get_feature_count_threshold(self):\n",
    "        return 10000\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _setup_training(self):\n",
    "        super()._setup_training()\n",
    "        self._theta = np.zeros(np.size(self._x, axis=1))\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _calculate_optimized_training_alg(self):\n",
    "        feature_count_small = self._get_num_features() < self.__get_feature_count_threshold()\n",
    "        if feature_count_small:\n",
    "            return RegressionAlgorithm.normal_equation\n",
    "        else:\n",
    "            return RegressionAlgorithm.gradient_descent\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _hypothesis(self):\n",
    "        return self._theta @ self._x.transpose()\n",
    "\n",
    "    # Override (abstract)\n",
    "    def _train(self,\n",
    "               training_algorithm=RegressionAlgorithm.unspecified,\n",
    "               learning_rate=0.01,\n",
    "               time_limit=None,\n",
    "               iteration_limit=None,\n",
    "               print_cost_while_training=False):\n",
    "        if training_algorithm == RegressionAlgorithm.gradient_descent:\n",
    "            self._train_with_gradient_descent(learning_rate, time_limit, iteration_limit, print_cost_while_training)\n",
    "        elif training_algorithm == RegressionAlgorithm.normal_equation:\n",
    "            self.__train_with_normal_equation()\n",
    "        else:\n",
    "            raise ValueError('Cannot start training, no linear regression algorithm specified.')\n",
    "\n",
    "    def __train_with_normal_equation(self):\n",
    "        x = self._x\n",
    "        x_trans = x.transpose()\n",
    "        y = self._y\n",
    "        regularization_matrix = np.identity(self._get_num_features())\n",
    "        regularization_matrix[0][0] = 0\n",
    "        regularization_matrix *= self._regularization_lambda\n",
    "        try:\n",
    "            result = np.linalg.inv(x_trans @ x + regularization_matrix) @ x_trans @ y\n",
    "        except ValueError as e:\n",
    "            raise Exception('Cannot calculate weights with normal equation.') from e\n",
    "        else:\n",
    "            self._theta = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionTrainerLogistic(RegressionTrainer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 coefficient_matrix,\n",
    "                 outputs,\n",
    "                 regularization_lambda=0.0,\n",
    "                 output_case_sensitive=True):\n",
    "        super().__init__(coefficient_matrix, outputs, regularization_lambda)\n",
    "        self.__output_case_sensitive = output_case_sensitive\n",
    "    \n",
    "    @property\n",
    "    def categories(self):\n",
    "        return self.__categories\n",
    "\n",
    "    def __get_num_categories(self):\n",
    "        return np.size(self.__categories)\n",
    "    \n",
    "    # Override\n",
    "    def _setup_training(self):\n",
    "        super()._setup_training()\n",
    "        unique_cat, b_output = DataProcessor.get_unique_categories_and_binary_outputs(self._y, self.__output_case_sensitive)\n",
    "        self.__categories = unique_cat\n",
    "        self._y = b_output\n",
    "        cat_count = self.__get_num_categories()\n",
    "        if cat_count < 2:\n",
    "            raise ValueError('Cannot do logistic regression, there is only one kind of output.')\n",
    "        elif cat_count == 2:\n",
    "            self.__binary_classification = True\n",
    "            self._theta = np.zeros(self._get_num_features())\n",
    "        else:\n",
    "            self.__binary_classification = False\n",
    "            theta_shape = (cat_count, self._get_num_features())\n",
    "            self._theta = np.zeros(shape=theta_shape)\n",
    "            \n",
    "    # Override (abstract)\n",
    "    def _calculate_optimized_training_alg(self):\n",
    "        return RegressionAlgorithm.gradient_descent\n",
    "    \n",
    "    # Override (abstract)\n",
    "    def _hypothesis(self):\n",
    "        theta_transpose_x = self._theta @ self._x.transpose()\n",
    "        result = np.zeros(shape=(self.__get_num_categories(), self._get_num_samples()))\n",
    "        result.fill(math.e)\n",
    "        result = result ** (-1 * theta_transpose_x)\n",
    "        result = 1/ (1 + result)\n",
    "        return result\n",
    "\n",
    "    # Override (abstract)\n",
    "    def _train(self,\n",
    "               training_algorithm=RegressionAlgorithm.unspecified,\n",
    "               learning_rate=0.01,\n",
    "               time_limit=None,\n",
    "               iteration_limit=None,\n",
    "               print_cost_while_training=False):\n",
    "        if training_algorithm == RegressionAlgorithm.gradient_descent:\n",
    "            self._train_with_gradient_descent(learning_rate, time_limit, iteration_limit, print_cost_while_training)\n",
    "        else:\n",
    "            raise ValueError('Cannot start training, no logistic regression algorithm specified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionPredictor(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, weights, feature_normalizer):\n",
    "        self._weights = weights\n",
    "        self._feature_normalizer = feature_normalizer\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def predict(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionPredictorLinear(RegressionPredictor):\n",
    "    # Override (abstract)\n",
    "    def predict(self, data):\n",
    "        normalized_feature = self._feature_normalizer.normalize_new_feature(data, False)\n",
    "        normalized_feature = DataProcessor.add_x0_column(normalized_feature)\n",
    "        return np.array(self._weights @ normalized_feature.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionPredictorLogistic(RegressionPredictor):\n",
    "    \n",
    "    # Override\n",
    "    def __init__(self, weights, feature_normalizer, categories):\n",
    "        super().__init__(weights, feature_normalizer)\n",
    "        self.__categories = categories\n",
    "\n",
    "    # Override (abstract)\n",
    "    def predict(self, data):\n",
    "        normalized_feature = self._feature_normalizer.normalize_new_feature(data, False)\n",
    "        normalized_feature = (DataProcessor.add_x0_column(normalized_feature)).astype(np.float64)\n",
    "        hypothesis = (self._weights).astype(np.float64) @ normalized_feature.transpose()\n",
    "        if np.size(self.__categories) <= 2:\n",
    "            return np.array([self.__categories[0] if p >= 0.5 else self.__categories[1] for p in hypothesis])\n",
    "        else:\n",
    "            max_ind = np.argmax(hypothesis, axis=0)\n",
    "            return np.array([self.__categories[ind] for ind in max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionSetup(metaclass=abc.ABCMeta):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 test_sample_ratio=0.05,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_lambda=0.0,\n",
    "                 regression_algorithm=RegressionAlgorithm.unspecified):\n",
    "        if data is None:\n",
    "            raise ValueError('Cannot initialize regression setup, no data.')\n",
    "        if not 0 <= test_sample_ratio < 1:\n",
    "            raise ValueError('Cannot initialize regression setup, invaild test sample ratio.')\n",
    "\n",
    "        self.data = data\n",
    "        self.test_sample_ratio = test_sample_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_lambda = regularization_lambda\n",
    "        self.regression_algorithm = regression_algorithm\n",
    "        \n",
    "    @abc.abstractproperty\n",
    "    def regression_type(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionSetupLinear(RegressionSetup):\n",
    "        \n",
    "    # Override (abstract)\n",
    "    @property\n",
    "    def regression_type(self):\n",
    "        return RegressionType.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegressionSetupLogistic(RegressionSetup):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 test_sample_ratio=0.05,\n",
    "                 learning_rate=0.01,\n",
    "                 regularization_lambda=0.0,\n",
    "                 regression_algorithm=RegressionAlgorithm.unspecified,\n",
    "                 output_case_sensitive=True):\n",
    "        super().__init__(data, test_sample_ratio, learning_rate, regularization_lambda, regression_algorithm)\n",
    "        self.output_case_sensitive = output_case_sensitive\n",
    "\n",
    "    # Override (abstract)\n",
    "    @property\n",
    "    def regression_type(self):\n",
    "        return RegressionType.logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Regression():\n",
    "    def __init__(self, setup):\n",
    "        self.__trained = False\n",
    "        self.__raw_data = setup.data\n",
    "        self.__reg_type = setup.regression_type\n",
    "        self.__test_sample_ratio = setup.test_sample_ratio\n",
    "        self.__learning_rate = setup.learning_rate\n",
    "        self.__regularization_lambda = setup.regularization_lambda\n",
    "        if self.__reg_type == RegressionType.logistic:\n",
    "            self.__output_case_sensitive = setup.output_case_sensitive\n",
    "        self.__reg_alg = setup.regression_algorithm\n",
    "        self.__setup_samples()\n",
    "        \n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            self.__trainer = RegressionTrainerLinear(coefficient_matrix=self.__x_training,\n",
    "                                                     outputs=self.__y_training,\n",
    "                                                     regularization_lambda=self.__regularization_lambda)\n",
    "            print('Linear trainer setup.')\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            self.__trainer = RegressionTrainerLogistic(coefficient_matrix=self.__x_training,\n",
    "                                                       outputs=self.__y_training,\n",
    "                                                       regularization_lambda=self.__regularization_lambda,\n",
    "                                                       output_case_sensitive=self.__output_case_sensitive)\n",
    "            print('Logistic trainer setup.')\n",
    "            \n",
    "    def train(self,\n",
    "              time_limit=None,\n",
    "              iteration_limit=None,\n",
    "              print_cost=False):\n",
    "        self.__trained = False\n",
    "        if not self.__trainer:\n",
    "            raise AttributeError('Cannot start training, trainer not found.')\n",
    "        self.__trainer.start_training(training_algorithm=self.__reg_alg,\n",
    "                                      learning_rate=self.__learning_rate,\n",
    "                                      time_limit=time_limit,\n",
    "                                      iteration_limit=iteration_limit,\n",
    "                                      print_cost_while_training=print_cost)\n",
    "        self.__trained = True\n",
    "        self.__setup_predictor()\n",
    "        self.__print_error_rate()\n",
    "\n",
    "    def predict(self, data):\n",
    "        if not self.__predictor:\n",
    "            raise Exception('Cannot predict, no predictor found.')\n",
    "        return (self.__predictor.predict(data)).flatten()\n",
    "\n",
    "    def __setup_samples(self):\n",
    "        if self.__raw_data is None:\n",
    "            raise ValueError('Cannot setup samples, no data.')\n",
    "        num_training_sample = self.__get_training_sample_count()\n",
    "        self.__x_training = self.__raw_data[:num_training_sample, :-1]\n",
    "        self.__y_training = self.__raw_data[:num_training_sample, -1]\n",
    "        self.__x_testing = self.__raw_data[num_training_sample:, :-1]\n",
    "        self.__y_testing = self.__raw_data[num_training_sample:, -1]\n",
    "        self.__preprocess_training_set_features()\n",
    "    \n",
    "    def __setup_predictor(self):\n",
    "        if not self.__trained:\n",
    "            raise Exception('Cannot setup predictor, model has not been trained.')\n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            self.__predictor = RegressionPredictorLinear(weights=self.__trainer.weights,\n",
    "                                                         feature_normalizer=self.__feature_normalizer)\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            self.__predictor = RegressionPredictorLogistic(weights=self.__trainer.weights,\n",
    "                                                           feature_normalizer=self.__feature_normalizer,\n",
    "                                                           categories=self.__trainer.categories)\n",
    "\n",
    "    def __print_error_rate(self):\n",
    "        error_rate = self.__get_testing_set_error_rate()\n",
    "        print('Error rate is {0:.2f}%.'.format(error_rate * 100))\n",
    "\n",
    "    def __get_training_sample_count(self):\n",
    "        total_sample_count = np.size(self.__raw_data, axis=0)\n",
    "        return math.ceil((1.0 - self.__test_sample_ratio) * total_sample_count)\n",
    "    \n",
    "    def __preprocess_training_set_features(self):\n",
    "        if self.__x_training is None:\n",
    "            raise ValueError('Cannot preprocess training set features, no data.')\n",
    "        self.__feature_normalizer = FeatureNormalizer(self.__x_training)\n",
    "        self.__x_training = self.__feature_normalizer.normalized_feature()\n",
    "        self.__x_training = DataProcessor.add_x0_column(self.__x_training)\n",
    "    \n",
    "    def __get_testing_set_error_rate(self):\n",
    "        if not self.__predictor:\n",
    "            raise Exception('Cannot get error rate, no predictor found.')\n",
    "        testing_sample_predictions = self.predict(self.__x_testing)\n",
    "        return self.__get_error_rate(testing_sample_predictions, self.__y_testing)\n",
    "            \n",
    "    def __get_error_rate(self, prediction, actual):\n",
    "        if self.__reg_type == RegressionType.linear:\n",
    "            diff = np.abs((prediction - actual)/actual)\n",
    "            diff = diff[~np.isnan(diff)]\n",
    "            return np.average(diff)\n",
    "        elif self.__reg_type == RegressionType.logistic:\n",
    "            match = prediction == actual\n",
    "            match_count = np.count_nonzero(match)\n",
    "            total_count = np.size(actual)\n",
    "            return (total_count - match_count) / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully queried data.\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "df = pd.read_csv('housing_data/housing.data', header=None, delim_whitespace=True)\n",
    "data_linear_reg = df.as_matrix()\n",
    "if data_linear_reg is not None:\n",
    "    print('Successfully queried data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLinear(data=data_linear_reg,\n",
    "                              test_sample_ratio=0.05,\n",
    "                              regularization_lambda=150,\n",
    "                              regression_algorithm=RegressionAlgorithm.normal_equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear trainer setup.\n",
      "Initializing......\n",
      "Setting up trainer......\n",
      "Training......\n",
      "Used 0.0003540516 seconds to train model with 481 samples and 13 features.\n",
      "Training finished.\n",
      "Error rate is 15.32%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ 16.8008594].\n",
      "Actual: 13.6.\n"
     ]
    }
   ],
   "source": [
    "features = np.matrix('0.10574   0.00  27.740  0  0.6090  5.9830  98.80  1.8681   4  711.0  20.10 390.11  18.07')\n",
    "actual = 13.6\n",
    "prediction = regression.predict(features)\n",
    "print('Prediction: {0}.\\nActual: {1}.'.format(prediction, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLinear(data=data_linear_reg,\n",
    "                              test_sample_ratio=0.05,\n",
    "                              learning_rate=0.01,\n",
    "                              regularization_lambda=150,\n",
    "                              regression_algorithm=RegressionAlgorithm.gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear trainer setup.\n",
      "Initializing......\n",
      "Setting up trainer......\n",
      "Training......\n",
      "Used 0.1644279957 seconds to train model with 481 samples and 13 features.\n",
      "Training finished.\n",
      "Error rate is 15.32%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train(time_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ 16.8008594].\n",
      "Actual: 13.6.\n"
     ]
    }
   ],
   "source": [
    "features = np.matrix('0.10574   0.00  27.740  0  0.6090  5.9830  98.80  1.8681   4  711.0  20.10 390.11  18.07')\n",
    "actual = 13.6\n",
    "prediction = regression.predict(features)\n",
    "print('Prediction: {0}.\\nActual: {1}.'.format(prediction, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully queried data.\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "df = pd.read_csv('iris_data/iris.data', header=None)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "data_logistic_reg = df.as_matrix()\n",
    "if data_logistic_reg is not None:\n",
    "    print('Successfully queried data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLogistic(data=data_logistic_reg,\n",
    "                                test_sample_ratio=0.1,\n",
    "                                learning_rate=0.1,\n",
    "                                regularization_lambda=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic trainer setup.\n",
      "Initializing......\n",
      "Setting up trainer......\n",
      "Training......\n",
      "Used 10.3965530396 seconds to train model with 135 samples and 4 features.\n",
      "Training finished.\n",
      "Error rate is 13.33%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train(time_limit=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['Iris-virginica'].\n",
      "Actual: Iris-virginica.\n"
     ]
    }
   ],
   "source": [
    "features = np.matrix('6.4,2.8,5.6,2.1')\n",
    "actual = 'Iris-virginica'\n",
    "prediction = regression.predict(features)\n",
    "print('Prediction: {0}.\\nActual: {1}.'.format(prediction, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundry with Two Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully queried data.\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "df = pd.read_csv('iris_data/iris.data', header=None)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "data_logistic_reg = df.as_matrix()\n",
    "# Make it a binary classification problem.\n",
    "data_logistic_reg[:, -1] = np.where(data_logistic_reg[:, -1] == 'Iris-setosa', 1, 0)\n",
    "# Reduct two only two features.\n",
    "iris_data_binary = np.delete(data_logistic_reg, np.s_[2:-1], axis=1)\n",
    "if iris_data_binary is not None:\n",
    "    print('Successfully queried data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class0 = iris_data_binary[np.where(iris_data_binary[:, -1] == 0)]\n",
    "class1 = iris_data_binary[np.where(iris_data_binary[:, -1] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA28AAANwCAYAAAC4T1KyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X+QXdlBH/jvsUZyBIMMjHsN2KDOEpyaARPvQokhVqFO\ntjTBNhj/gZINu2FNKsQh0mwmBopVykZSZsPkB/EIaoSJF1VlTNbtRdlkAgYqDsm8mcws9mixDWuP\nNkCCFGLAbvwDDNZOa57P/nGfLHVbP/rH1Xv3PH0+VV2nz+n77j3n3JbUX51zX5daawAAABi2F8y6\nAwAAANyc8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwB3CZKKftLKU+XUj5V\nSvn9Usq/L6V8wwZf+9lSyn99q/t4q5VS9k7G8u517T9dSvnhWfVrvVLKgVLKb8+6HwAMi/AGcBso\npXxRkp9L8mNJviTJS5OcSPLcBk9Rb1HXZuWbSin3zroTN1Ayf3MOwDYJbwC3h5cnqbXWn6md52qt\nv1Rr/dDlA0opf7WU8mwp5eOllF8spXzlpP2JdGHi10opf1hKOTRp/95Sym9MVvEeK6V8+VXneriU\n8tFSyh+UUn61lHLPpP01pZT3T9ovlFKOXa/Dk7685qr6jlLKx0opryylvHCyWvb7pZRPllLeV0pZ\n2MR8/MMkP3KDa99obJ8tpbyxlPLrpZRPlFIeudGFbjAXu0opPzqZh98tpbxtMq4vSPILSb6ilPLp\nyZx/2eT4k6WUj5RS/svkvDsn57qrlPJzk7n4+OSeXb7+D5VSfnNyng+VUl6/iXkCYECEN4Dbw68n\nGZdS/mkp5VtLKV989RdLKd+R5H9J8vokC0n+fZJ3JUmt9cDksFfUWvfUWs+UUv58uvDznUm+PMl/\nvnx8KeW+JPuT/Kla64uS/MUkH5+c44+S/JVJ+2uT/I1Syuuu0+d3Jvmuq+rfmmSl1vrBJP9Tkj3p\nVhC/NMnfSHJxg3NRk/xEkpdPxrHGjcZ2ldcm+YYkfybJX5yM+fPcZC7+QZI/leTrJ+VXJPnhWutn\nkrw6ye/UWr9oMue/l+TNSfZNjv8zk8/fPDnX9yf57SR3Jfmvkvydq7rxm0leVWvdk2619Z+VUl5y\ns0kCYHiEN4DbQK310+lCxGeTvD3Jx0op/+qq1ao3Jnmo1vrrtdbPJvn7SV55efVtolz1+XclOV1r\n/dVa66UkR5PcW0r5qiSXknxRkntKKaXW+h9qrR+d9OPJWuuHJ59/KF0oOpBrW07yulLKn5jU//Kk\nLZNr3JXk5ZOVxA/UWv9oE1NyMcnfS/K/XuNr1xrbN0/GdtlDtdZP11p/O8njSV55netcdy6SfG+S\nv11r/YNa6x+nm/O/fIM+f1eSE7XWj9daP54uiP2Vq67z5Un+ZK11XGt9+vKLaq3/51XzfybJb6QL\nfgA0RngDuE1MgsNfrbV+VZKvS7fSc3Ly5b1JfmyyDfAT6VaHarqVrWv5iiQXrjr3Hyf5RJKX1lof\nT/JIklNJPlpK+clSyp1JUkrZV0r5d5Ptj59KFxpffJ3+/sckzyb59lLK7iSvS7calyQ/neRfJ3nX\nZAvh3y+l7NjklPxUkpeUUr5tA2P7eNbOxUev+vwzSS6P70NXbXV81fXmYhKavyDJr1w157+YLpBe\nz1ekWwW87MKkLUn+UZL/mOQ9ky2SP3T5oFLKd5dSPjDZUvnJJF+b68w5AMMmvAHchmqtv57kn6YL\ncUm35e6NtdYvnXx8Sa31zlrre69zit9JF/iSJKWUL0wXPD4yOf8jtdZvTHJPkj+d5Acnh74zyWPp\nQt4XJ/knWbuit9670q04fUeSD9da/9Pk/M/XWh+stX5tkj+b5NuTfPcm5+BSutWrBzc4tv+ygXN+\n3VVbHZ+etF1rLn4/Xej72qvm/IsnWyuTa79ZyUeu7tfk89+ZXOOPaq0/UGv96nQh902llD83WS18\ne5K/ObmnX5Lkw7nxnAMwUMIbwG2glPKnSylvKqW8dFL/ynRb9H55cshPJvk7V72ZxotKKd951Sl+\nL8nVvypgOcn3lFK+vpTywnTPiP1yrfU/l1K+cbLCdke67Yn/X5Lx5HV3JvlkrfVSKWVf1j7Tdi3v\nSnJfku/LlVW3lFKWSilfV0p5Qbrn6C6l2xK6oem46vN/luRPpHvG7EZje+9ki+SmXGcuPltrrUn+\ntyQnL29dLaW89Kpn5z6a5K5Syp6rTveuJG8upby4lPLiJG9JtwKZUsprSylfPTnu00meTzcfXzgp\nf7+U8oJSyvfkSmAHoDHCG8Dt4dNJvinJ+0opn07yfyX5tSQ/kCS11sfSPXP1rsl2xl9L9wYhlx1P\n8o7JFr/vrLX+23Th4V+kWxH6k7nyvNaedMHkE0l+K90q049OvvY3kzxYSvmDdG+28X/cqNOTN+r4\n5ST3rjv2y5L88yR/kG4l6fFcCTJvK6X8xI1Oe9X5P5vkh9P9+oQ6abvW2P77a73+OvWrXWsu/tHk\naz+U7s1E3juZ8/eke1fQ1Fr/Q7oQ+Z8mc/5l6Z7P+7/T3ZtfnXz+9ybn+pokvzS5t08nOVVrfaLW\nei7JP07y3nQB/GuTPHWD/gIwYKX7z7+bHFTK+XT/QH42yaVa6+c96FxK+fF0/3P5x0neMHk3MAAA\nAHpwxwaP+2ySpVrrJ6/1xVLKq5N8da31a0op35Ru+82Qf/kpAABAUza6bbLc5NjvSPKOJKm1vi/J\ni/wOGQAAgP5sNLzVJP+mlHK2lPK91/j6S9O9U9llH8n1314aAACATdrotslX1Vp/d/KOWP+mlHKu\n1uqBZwAAgCnZUHirtf7upFwppfzLJPuy9t2qPpLkK6+qv2zStkYp5ebvjgIAADDHaq1b+n2bN902\nWUr5glLKnZPPvzDd79v50LrDfjaTX45aSrk3yadqrR+9Tkd9TPHj2LFjM+/D7fZhzs357fBhzs35\n7fBhzs357fBhzqf/sR0bWXl7SZJ/OVk1uyPJ/15rfU8p5Y1dFqtvr7X+QinlNaWU30z3qwK+Z1u9\nAgAAYI2bhrda628leeU12v/JuvqRHvsFAADAVTb6bpM0amlpadZduO2Y8+kz59NnzqfPnE+fOZ8+\ncz595rwtZbv7Ljd1sVLqNK8HAAAwJKWU1Fv1hiUAAADMnvAGAADQAOENAACgAcIbAABAA4Q3AACA\nBghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q\n3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwB\nAAA0QHgDAABogPAGAADQAOENAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAA\naIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA\n4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOENAACgAcIb\nAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAA\ngAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAAN\nEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOENAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8\nAQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABogvAEAADRAeAMA\nAGiA8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQ\nAOENAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHC\nGwAAQAOENwAAgAZsOLyVUl5QSnl/KeVnr/G1A6WUT02+/v5Sypv77SYAAMDt7Y5NHPu3kjybZM91\nvv5krfV12+8SAAAA621o5a2U8rIkr0nyUzc6rJceAQAA8Hk2um3y4SQ/mKTe4JhvLqV8sJTy86WU\ne7bfNQAAAC676bbJUsprk3y01vrBUspSrr3C9itJvqrW+plSyquTPJbk5dc63/Hjxz/3+dLSUpaW\nljbfawAAgAaMRqOMRqNezlVqvdFiWlJK+ZEk/2OS55PsTvJFSf5FrfW7b/Ca30ryDbXWT6xrrze7\nHgAAwLwqpaTWuqVHzm4a3tZd6ECS71//xiSllJfUWj86+Xxfkp+ptS5e4/XCGwAAcNvaTnjbzLtN\nrr/oG5PUWuvbk3xnKeX7klxKcjHJX9rqeQEAAPh8m1p52/bFrLwBAAC3se2svG34l3QDAAAwO8Ib\nAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAA\ngAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q2gR+PxjesAAFslvAH0ZDxO9u9PRqOu\nPhp1dQEOAOjDHbPuAMC82LEjeeih5NCh5PDh5NSp5MyZrh0AYLusvAH0aGmpC24nTnTl0tKsewQA\nzAvhDaBHo1G34nbsWFde3kIJALBdpdY6vYuVUqd5PYBpuvzM20MPdStuo1Fy9Gjy1FO2TgIAnVJK\naq1lS68V3gD6Mx6vDWrr6wDA7W074c22SYAerQ9qghsA0BfhDQAAoAHCGwAAQAOENwAAgAYIbwAA\nAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAa\nILwBAAA0QHgDAABogPAGAADQAOENAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4\nAwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYA\nANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOENaNZ4\nfOM6AMA8Ed6AJo3Hyf79yWjU1Uejri7AAQDz6o5ZdwBgK3bsSB56KDl0KDl8ODl1KjlzpmsHAJhH\nVt6AZi0tdcHtxImuXFqadY8AAG4d4Q1o1mjUrbgdO9aVl7dQAgDMo1Jrnd7FSqnTvB4wvy4/8/bQ\nQ92K22iUHD2aPPWUrZMAwHCVUlJrLVt6rfAGtGo8XhvU1tcBAIZmO+HNtkmgWeuDmuAGAMwz4Q0A\nAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOENAACgAcIbAABA\nA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDebYeHzjOgAA7RDeYE6Nx8n+\n/clo1NVHo64uwAEAtOmOWXcAuDV27Egeeig5dCg5fDg5dSo5c6ZrBwCgPVbeYI4tLXXB7cSJrlxa\nmnWPAADYKuEN5tho1K24HTvWlZe3UAIA0J5Sa53exUqp07we3M4uP/P20EPdittolBw9mjz1lK2T\nAACzUkpJrbVs6bXCG8yv8XhtUFtfBwBgurYT3mybhDm2PqgJbgAA7RLeAAAAGiC8AQAANEB4AwAA\naIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA\n4Q0AAKABwhsAAEADNhzeSikvKKW8v5Tys9f5+o+XUn6jlPLBUsor++siAAAAm1l5+1tJnr3WF0op\nr07y1bXWr0nyxiQ/2UPfAAAAmNhQeCulvCzJa5L81HUO+Y4k70iSWuv7kryolPKSXnoIcJsaj29c\nBwBuLxtdeXs4yQ8mqdf5+kuT/PZV9Y9M2gDYgvE42b8/GY26+mjU1QU4ALh93XGzA0opr03y0Vrr\nB0spS0nKLe8VwG1ux47koYeSQ4eSw4eTU6eSM2e6dgDg9nTT8JbkVUleV0p5TZLdSb6olPKOWut3\nX3XMR5J85VX1l03aPs/x48c/9/nS0lKWlpY22WWA28PSUhfcTpxIjh3r6gBAW0ajUUaXt9JsU6n1\nejshr3FwKQeSfH+t9XXr2l+T5HCt9bWllHuTnKy13nuN19fNXA/gdjYaff7KmwAHAG0rpaTWuqXd\njBtZebveRd+YpNZa315r/YVSymtKKb+Z5I+TfM9WzwtA92zb0aNXAtvSUld/6ilbJwHgdrWplbdt\nX8zKG8CGjcdrg9r6OgDQnu2svG3m97wBMEXrg5rgBgC3N+ENAACgAcIbAABAA4Q3AACABghvAAAA\nDRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABog\nvAEAADRAeAMAAGiA8AYAANAA4Q2YutXVG9cBAPh8whswVauryV13JSdPdvWTJ7u6AAcAcGN3zLoD\nwO1l167kwQeTN70peeyx5Mknk7e+tWsHAOD6Sq11ehcrpU7zesBwLS0lTzyRHDiQjEaz7g0AwHSU\nUlJrLVt5rW2TwNSdPNmtuB040JWXt1ACAHB9Vt6Aqbr8zNuDDyYPPNAFt7e8Jfn4x22dBADm33ZW\n3oQ3YOpWV9cGtfV1AIB5JbwBAAA0wDNvAAAAc054AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAA\ngAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAAN\nEN4AAAAaILzBAF28eOM6wzUe37gOQENWVpKzZ7tySIbaL2454Q0G5uLF5M47kyNHuvqRI11dgBu+\n8TjZvz8Zjbr6aNTVBTiABi0vJ3v3JgcPduXy8qx71Blqv5iKUmud3sVKqdO8HrTqyJHk1Knu7+QL\nF5LDh5NHHpl1r9iI0Sg5dKi7Z6dOJWfOJEtLs+4VAJuystL9I3z1/5zu3t39o7ywoF9sSykltday\nlddaeYMBeuSRK8Ft717BrSVLS11wO3GiKwU3gAadP5/s2rW2befOrn2WhtovpkZ4gwE6cuRKcLtw\n4coWSoZvNOpW3I4d68rLWygBaMjiYrK6urbt0qWufZaG2i+mRniDgbl4MXnb27pVm/Pnu/Jtb/PM\nWwvG4+To0W6r5PHjXXn0qGfeAJqzsJCcPt1tSdyzpytPn5791sSh9oup8cwbDNDFi93fx9erM1zj\ncbJjx/XrADRkZaX7n9TFxWEFpKH2iw3ZzjNvwhsAAMCUeMMSAACAOSe8AQAANEB4AwAAaIDwBgAA\n0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKAB\nwhsAAEADhDcAAIAGCG8AAAANEN5ggMbjG9dnoc8+DXF8AABDJ7zBwIzHyf79yWjU1Uejrj7LgNNn\nn4Y4PgCAFtwx6w4Aa+3YkTz0UHLoUHL4cHLqVHLmTNc+D30a4vgAAFpg5Q0GaGmpCzYnTnTl0tKs\ne9Rvn4Y4PgCAoRPeYIBGo25F6tixrry8xXCW+uzTEMcHADB0pdY6vYuVUqd5PWjR5WfCHnqoW5Ea\njZKjR5Onnprd1sI++zTE8QEATEspJbXWsqXXCm8wPOPx2iCzvj4LffZpiOMDAJgG4Q0AAKAB2wlv\nnnkDAABogPAGAADQAOENAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDw\nBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABogvAEAADRAeOOmxuMb1+mYJwBuqZWV\n5OzZrgRuS8IbNzQeJ/v3J6NRVx+NurpgspZ5AuCWWl5O9u5NDh7syuXlWfcImIFSa53exUqp07we\n/RiNkkOHksOHk1OnkjNnkqWlWfdqeMwTALfEykoX2C5evNK2e3dy4UKysDC7fgFbUkpJrbVs5bVW\n3rippaUukJw40ZUCybWZJwBuifPnk1271rbt3Nm1A7cV4Y2bGo26laRjx7ry8tZA1jJPANwSi4vJ\n6uratkuXunbgtiK8cUPjcXL0aLcF8Pjxrjx61LNc65knAG6ZhYXk9Oluq+SePV15+rQtk3Ab8swb\nNzUeJzt2XL9OxzwBcEutrHRbJRcXBTdo2HaeeRPeAAAApsQblgAAAMw54Q0AAKABwhsAAEADhDcA\nAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOENAACgAcIbAABAA4Q3AACABghvAAAA\nDRDeAAAAGnDT8FZKeWEp5X2llA+UUj5cSvmRaxxzoJTyqVLK+ycfb7413QUAALg93XGzA2qtz5VS\n/lyt9TOllB1Jni6lvKrW+vS6Q5+stb7u1nQTAADg9rahbZO11s9MPn3h5DWfvMZhpa9OQYtWV29c\nn8W5xuMb11s37+MDALjahsJbKeUFpZQPJPm9JKNa67PXOOybSykfLKX8fCnlnl57CQO3uprcdVdy\n8mRXP3myq28ldPV1rvE42b8/GY26+mjU1ecl4Mz7+AAA1rvptskkqbV+Nsl/U0rZk+Q9pZQDtdYn\nrjrkV5J81WRr5auTPJbk5f13F4Zp167kwQeTN70peeyx5Mknk7e+tWuf1bl27Egeeig5dCg5fDg5\ndSo5c6ZrnwfzPj4AgPVKrXVzLyjlLUk+U2v9xzc45reSfEOt9RPr2uuxY8c+V19aWsrS0tKmrg9D\ntrSUPPFEcuDAlRWhWZ/r+PHkxInk2LHu83kz7+MDANo2Go0yuuqHuRMnTqTWuqVHzm4a3kopL05y\nqdb6B6WU3Un+dZITtdZ/e9UxL6m1fnTy+b4kP1NrXbzGuepmwyK04uTJbrXsW77lymrZAw/M9lyj\n0eevTM3T/5fM+/gAgPlTSrml4e0VSR5N94YkL0jy07XWHy2lvDFJrbW+vZRyOMn3JbmU5GKSv11r\nfd81ziW8MZcuP6f24INdyDp5MnnLW5KPf3zz2x37OtflZ8IeeqgLNKNRcvRo8tRT87G1cN7HBwDM\np1sa3vokvDHPVlfXhqv19VmcazxeG2TW11s37+MDAOaP8AYAANCA7YS3Df2qAAAAAGZLeAMAAGiA\n8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOEN\nAACgAcIbAABAA4Q3AACABghvAAAADRDeuKnx+Mb1WVhdvXF9FvrsU19zPsR7lwzz/rEJKyvJ2bNd\nORR99mmI4wOACG/cxHic7N+fjEZdfTTq6rMMAauryV13JSdPdvWTJ7v6LANAn33qa86HeO+SYd4/\nNmF5Odm7Nzl4sCuXl2fdo377NMTxAcBltdapfXSXozWPP17ri19c67FjXfn44zPuUK314YdrLaXW\nAwe68uGHZ92jfvvU15wP8d7VOsz7xwZ87GO17t5da3LlY/furn0e+jTE8QEwdyaZaEt5ysobN7W0\nlBw+nJw40ZVLS7PuUfLAA8m3fEvyxBNd+cADs+5Rv33qa86HeO+SYd4/NuD8+WTXrrVtO3d27bPS\nZ5+GOD4AuIrwxk2NRsmpU8mxY115eRveLJ08mTz5ZHLgQFde3oI3L33qa86HeO+SYd4/NmBx8fP3\nt1661LXPSp99GuL4AOBqW12y28pHbJtszvPP13rvvVe22z3+eFd//vnZ9em552q9884rW+0efrir\nP/fcfPSprzkf4r2rdZj3j0145zu7rYR79nTlO9856x7126chjg+AuZJtbJss3euno5RSp3k9+jEe\nJzt2XL8+C6ura3c3ra/PQp996mvOh3jvkmHePzZhZaXbSri4mCwszLo3nT77NMTxATA3SimptZYt\nvVZ4AwAAmI7thDfPvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4A\nAAAaILwBAAA0QHgDAABogPAGAADQAOENAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8QU/G\n4xvXZ3UuYJPOnUsefbQruT7zBDB1whv0YDxO9u9PRqOuPhp19a2Erj7PBWzS/fcn99yTvOENXXn/\n/bPu0TCZJ4CZKLXW6V2slDrN68E0jUbJoUPJ4cPJqVPJmTPJ0tLszwVs0LlzXRBZ79lnk7vvnn5/\nhso8AWxLKSW11rKV11p5g54sLXVh68SJrtxO2OrzXMAGPfPM5tpvV+YJYGaEN+jJaNStkh071pWX\ntz3O+lzABu3bt7n225V5ApgZ4Q16MB4nR4922xuPH+/Ko0e3/sxbX+cCNuHuu5MjR9a2HTliK+B6\n5glgZjzzBj0Zj5MdO65fn9W5gE06d67bArhvn0ByI+YJYEu288yb8AYAADAl3rAEAABgzglvAAAA\nDRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABog\nvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEADhLeBGY9vXJ+Fvvq0unrjeuuGeO9gjZWV5OzZ\nrmQ6zp1LHn20K7dj3u9dn+Pr61xDnPMh9gmYKuFtQMbjZP/+ZDTq6qNRV59lCOirT6uryV13JSdP\ndvWTJ7v6vAS4Id47WGN5Odm7Nzl4sCuXl2fdo/l3//3JPfckb3hDV95//9bOM+/3rs/x9XWuIc75\nEPsETF2ptU7vYqXUaV6vRaNRcuhQcvhwcupUcuZMsrQ0H306eTJ505uSb/mW5Mknk7e+NXnggb57\nOztDvHeQpPtf+r17k4sXr7Tt3p1cuJAsLMyuX/Ps3LkusK337LPJ3Xdv/Dzzfu/6HF9f5xrinA+x\nT8CWlVLahpZRAAAgAElEQVRSay1bea2Vt4FZWup++D9xoiuH8MN/X3164IEuuD3xRFfOU3BLhnnv\nIEly/nyya9fatp07u3ZujWee2Vz79cz7vetzfH2da4hzPsQ+ATMhvA3MaNSt2hw71pWXt+HNUl99\nOnmyW3E7cKArL2+hnBdDvHeQJFlc/Pw9ypcude3cGvv2ba79eub93vU5vr7ONcQ5H2KfgJkQ3gZk\nPE6OHu222x0/3pVHj87+mbc++rS6mrzlLd1WydGoK9/ylvl65m1o9w4+Z2EhOX2622a1Z09Xnj5t\nu9WtdPfdyZEja9uOHNnclslk/u9dn+Pr61xDnPMh9gmYCc+8Dcx4nOzYcf36LPTVp9XVtbs+1tdb\nN8R7B2usrHTbrBYX/dA3LefOdVsl9+3bfHC72rzfuz7H19e5hjjnQ+wTsGnbeeZNeAMAAJgSb1gC\nAAAw54Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAA\ngAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhs3NR7fuD4Lffbp4sUb1wG2\nbGUlOXu2K+fN008nx4515ZAMtV8APRDeuKHxONm/PxmNuvpo1NVnGeD67NPFi8mddyZHjnT1I0e6\nugAHbNvycrJ3b3LwYFcuL8+6R/25777uL96/+3e78i/8hVn3qDPUfgH0pNRap3exUuo0r0c/RqPk\n0KHk8OHk1KnkzJlkaWl++nTkSHeOvXuTCxe6cz7ySJ+9BW47KyvdXypX/0/Q7t3dXzILC7PrVx+e\nfroLRus99VTyqldNvz+XDbVfAOuUUlJrLVt5rZU3bmppqQs0J0505ayDW9Jvnx555Epw27tXcAN6\ncP58smvX2radO7v21r3nPZtrn5ah9gugR8IbNzUadStTx4515eXtirPUZ5+OHLkS3C5cuLKFEmDL\nFheT1dW1bZcude2tu+++zbVPy1D7BdAj4Y0bGo+To0e7bYnHj3fl0aOzf+atrz5dvJi87W3d6t35\n8135trd55g3YpoWF5PTpbqvknj1defp0+1smk24L4vpAdN99s9+aONR+AfTIM2/c1Hic7Nhx/fos\n9Nmnixe7n6uuVwfYspWV7n+GFhfnI7hd7emnuy2JQwtIQ+0XwMR2nnkT3gAAAKbEG5YAAADMOeEN\nAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAA\nQAOENwAAgAYIbwAAAA0Q3gAAABpw0/BWSnlhKeV9pZQPlFI+XEr5kesc9+OllN8opXywlPLK/rsK\nAABw+7rjZgfUWp8rpfy5WutnSik7kjxdSnlVrfXpy8eUUl6d5KtrrV9TSvmmJD+Z5N5b120AAIDb\ny4a2TdZaPzP59IWT13xy3SHfkeQdk2Pfl+RFpZSX9NXJoRuPb1yf1bnm2erqjeub0decu3ebtLKS\nnD3blUNx7lzy6KNduR1DHFsyzPH11ad59/TTybFjXbldfd2/Pu/dUP/M9KWv8c37PPXJnDOvaq03\n/UgX2D6Q5A+T/MNrfP3nkvzZq+q/lOS/vcZxdd48/3yt995b6+OPd/XHH+/qzz8/23PNs+eeq/XO\nO2t9+OGu/vDDXf255zZ/rr7m3L3bpHe+s9bdu2t90Yu68p3vnHWPaj1ypNbkyseRI1s7zxDHVusw\nx9dXn+bdwYNr5+m++7Z+rr7uX5/3bqh/ZvrS1/jmfZ76ZM4ZuEkm2lAOW/+xuYOTPUnem+TAuvbb\nNrzV2v2g/uIX13rsWFde/gF+1ueaZw8/XGsptR440JWXg9xW9DXn7t0Gfexj3T+CV//gt3t31z4r\nzz67tj+XP559dnPnGeLYah3m+Prq07x76qlrz9NTT23+XH3dvz7v3VD/zPSlr/HN+zz1yZzTgO2E\nt5s+87Zule4PSyk/n+Qbkzxx1Zc+kuQrr6q/bNL2eY4fP/65z5eWlrK0tLSZLgzS0lJy+HBy4kS3\nq2U7Q+rzXPPsgQeSxx5LnngiOXCgq29VX3Pu3m3Q+fPJrl3JxYtX2nbu7NoXFmbTp2eeuX773Xdv\n/DxDHFsyzPH11ad59573XL/9Va/a3Ln6un993ruh/pnpS1/jm/d56pM5Z4BGo1FGo1E/J7tZukvy\n4iQvmny+O8mTSf67dce8JsnPTz6/N8l7r3OuWx1kZ8LK2/RZeWvYEP83c4grU30a4visvG2Mlbe2\nWQWaPnNOA3Irt00meUWS96d75u1Xk/zApP2NSf76Vcc9kuQ3J8d83pbJOqfhzTNv0+eZtzlw+TmC\nPXuG8xxB38+EDWlstQ5zfJ5525j77ls7T30887bd+3crnnkb2p+ZvvQ1vnmfpz6ZcwZuO+GtdK+f\njlJKneb1pmU8TnbsuH59VueaZ6ur3W6G69U3o685d+82aWWl236yuDicLSjnznVbv/bt297WvSGO\nLRnm+Prq07x7+uluq+R9921+u+R6fd2/Pu/dUP/M9KWv8c37PPXJnDNgpZTUWsuWXiu8AQAATMd2\nwtuGfs8bAAAAsyW8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAA\nABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEADhDcAAIAGCG8AAAANEN4GZjy+cR24hVZW\nkrNnu3II5xmqPsc3xDkf6rn6MsQ+MX2+D6BJwtuAjMfJ/v3JaNTVR6OuLsDBFCwvJ3v3JgcPduXy\n8mzPM1R9jm+Icz7Uc/VliH1i+nwfQLNKrXV6FyulTvN6LRqNkkOHksOHk1OnkjNnkqWlWfcK5tzK\nSvcDzMWLV9p2704uXEgWFqZ/nqHqc3xDnPOhnqsvQ+wT0+f7AGaulJJaa9nKa628DczSUhfcTpzo\nSsENpuD8+WTXrrVtO3d27bM4z1D1Ob4hzvlQz9WXIfaJ6fN9AE0T3gZmNOpW3I4d68rLWyiBW2hx\nMVldXdt26VLXPovzDFWf4xvinA/1XH0ZYp+YPt8H0DThbUDG4+To0W6r5PHjXXn0qGfe4JZbWEhO\nn+62Du3Z05WnT29+C1Ff5xmqPsc3xDkf6rn6MsQ+MX2+D6BpnnkbmPE42bHj+nXgFlpZ6bYOLS5u\n7weZvs4zVH2Ob4hzPtRz9WWIfWL6fB/AzGznmTfhDQAAYEq8YQkAAMCcE94AAAAaILwBAAA0QHgD\nAABogPAGAADQAOENAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA\n0ADhDQAAoAHCGwAAQAOENwAAgAYIbwCXrawkZ8925XacO5c8+mhXDqVPQ9XXXPU5T32ea4jjG6J5\nH9+8m+f7N89jo0nCG0CSLC8ne/cmBw925fLy1s5z//3JPfckb3hDV95//+z7NFR9zVWf89TnuYY4\nviGa9/HNu3m+f/M8NppVaq3Tu1gpdZrXA9iQlZXuH+aLF6+07d6dXLiQLCxs/DznznU/pK/37LPJ\n3XfPpk9D1ddc9TlPfZ5riOMbonkf37yb5/s3z2Nj5kopqbWWrbzWyhvA+fPJrl1r23bu7No345ln\nNtc+jT4NVV9z1ec89XmuIY5viOZ9fPNunu/fPI+NpglvAIuLyerq2rZLl7r2zdi3b3Pt0+jTUPU1\nV33OU5/nGuL4hmjexzfv5vn+zfPYaJrwBrCwkJw+3W2J2bOnK0+f3vzWmLvvTo4cWdt25Mjmt0z2\n2aeh6muu+pynPs81xPEN0byPb97N8/2b57HRNM+8AVy2stJtiVlc3N4/0OfOddvj9u3bWnC7FX0a\nqr7mqs956vNcQxzfEM37+ObdPN+/eR4bM7OdZ96ENwAAgCnxhiUAAABzTngDAABogPAGAADQAOEN\nAACgAcIbAABAA4Q3AACABghvAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAA\nQAOENwAAgAYIbwAAAA0Q3gAAABogvMG8W1lJzp7tSm5siHP17ncnf+2vdeVQDHGe+tTn+OZ9rgCY\nKuEN5tnycrJ3b3LwYFcuL8+6R8M1xLl6xSuSb//25PTprvz6r591j4Y5T33qc3zzPlcATF2ptU7v\nYqXUaV4PbmsrK90PjBcvXmnbvTu5cCFZWJhdv4ZoiHP17nd3gW29n/u55Nu+bfr9SYY5T33qc3zz\nPlcAbFkpJbXWspXXWnmDeXX+fLJr19q2nTu7dtYa4lw99tjm2qdhiPPUpz7HN+9zBcBMCG8wrxYX\nk9XVtW2XLnXtrDXEuXr96zfXPg1DnKc+9Tm+eZ8rAGZCeIN5tbDQPSu1e3eyZ09Xnj5ty9a1DHGu\nvu3bumfervaKV8xuy2QyzHnqU5/jm/e5AmAmPPMG825lpduqtbjoB8ebGeJcvfvd3VbJ179+tsHt\nakOcpz71Ob55nysANm07z7wJbwAAAFPiDUsAAADmnPAGAADQAOENAACgAcIbAABAA4Q3AACABghv\nAAAADRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAA\nABogvAEAADRAeAPatrKSnD3blUPRZ5+GOL4+zfv4gLb5O4qBEd6Adi0vJ3v3JgcPduXy8qx71G+f\nhji+Ps37+IC2+TuKASq11uldrJQ6zesBc2xlpfvH9OLFK227dycXLiQLC+33aYjj69O8jw9om7+j\nuIVKKam1lq281sob0Kbz55Ndu9a27dzZtc9Kn30a4vj6NO/jA9rm7ygGSngD2rS4mKyurm27dKlr\nn5U++zTE8fVp3scHtM3fUQyU8Aa0aWEhOX2628ayZ09Xnj492+0sffZpiOPr07yPD2ibv6MYKM+8\nAW1bWem2sSwuDucf1T77NMTx9Wnexwe0zd9R3ALbeeZNeAMAAJgSb1gCAAAw54Q3AACABghvAAAA\nDRDeAAAAGiC8AQAANEB4AwAAaIDwBgAA0ADhDQAAoAHCGwAAQAOENwAAgAYIbwAAAA0Q3gAAABog\nvAEAADRAeAMAAGjATcNbKeVlpZR/V0r5cCnl/yml/M/XOOZAKeVTpZT3Tz7efGu6CwAAcHu6YwPH\nPJ/kTbXWD5ZS7kzyK6WU99Ra/991xz1Za31d/10EAADgpitvtdbfq7V+cPL5HyU5l+Sl1zi09Nw3\nmJ6VleTs2a4ciiH2aYj6nKe+zuXete3cueTRR7uS6/N9DjB1m3rmrZSymOSVSd53jS9/cynlg6WU\nny+l3NND32A6lpeTvXuTgwe7cnl51j0aZp+GqM956utc7l3b7r8/ueee5A1v6Mr77591j4bJ9znA\nTJRa68YO7LZMjpI8WGv9V9f42mdrrZ8ppbw6yY/VWl9+jXPUjV4PpmJlpfvB4+LFK227dycXLiQL\nC/o0ZH3OU1/ncu/adu5cF9jWe/bZ5O67p9+fofJ9DrAtpZTUWre0a3Ejz7yllHJHkn+e5KfXB7fk\nc9spL3/+i6WUnyilfGmt9RPrjz1+/PjnPl9aWsrS0tIWug09OX8+2bVr7Q8hO3d27bP6IWSIfRqi\nPuepr3O5d2175pnrtwtvV/g+B9iU0WiU0WjUy7k2tPJWSnlHkt+vtb7pOl9/Sa31o5PP9yX5mVrr\n4jWOs/LGsAzxf5CH2KchsvJG36y8bYzvc4Bt2c7K20Z+VcCrkvwPSf58KeUDk18F8K2llDeWUv76\n5LDvLKV8qJTygSQnk/ylrXQGpm5hITl9uvvBY8+erjx9erY/gAyxT0PU5zz1dS73rm13350cObK2\n7cgRwW093+cAM7PhZ956uZiVN4ZqZaXb8rO4OJwfQIbYpyHqc576Opd717Zz57qtkvv2CW434vsc\nYEu2s/ImvAEAAEzJLd02CQAAwOwJbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKAB\nwhsAAEADhDcAAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOENAACgAcIbAABAA4Q3\nSJKVleTs2a6kLefOJY8+2pUAAHNMeIPl5WTv3uTgwa5cXp51j9io++9P7rknecMbuvL++2fdIwCA\nW6bUWqd3sVLqNK8HN7Wy0gW2ixevtO3enVy4kCwszK5f3Ny5c11gW+/ZZ5O7755+fwAANqCUklpr\n2cprrbxxezt/Ptm1a23bzp1dO8P2zDObawcAaJzwxu1tcTFZXV3bdulS186w7du3uXYAgMYJb9ze\nFhaS06e7rZJ79nTl6dO2TLbg7ruTI0fWth05YsskADC3PPMGSffs2/nz3Yqb4NaWc+e6rZL79glu\nAMDgbeeZN+ENAABgSrxhCQAAwJwT3gAAABogvAEAADRAeAMAAGiA8AYAANAA4Q0AAKABwhsAAEAD\nhDcAAIAGCG8AAAANEN4AAAAaILwBAAA0QHgDAABogPAGAADQAOENAACgAcIbAABAA4S3OTUe37jO\nLbKykpw925VMhzmnb76nABgo4W0OjcfJ/v3JaNTVR6OuLsDdYsvLyd69ycGDXbm8POsezT9zTt98\nTwEwYKXWOr2LlVKneb3b2WiUHDqUHD6cnDqVnDmTLC3NuldzbGWl+0Hv4sUrbbt3JxcuJAsLs+vX\nPDPn9M33FABTUEpJrbVs5bVW3ubU0lIX3E6c6ErB7RY7fz7ZtWtt286dXTu3hjmnb76nABg44W1O\njUbdituxY115eQslt8jiYrK6urbt0qWunVvDnNM331MADJzwNofG4+To0W6r5PHjXXn0qGfebqmF\nheT06W6L1Z49XXn6tK1Wt5I5p2++pwAYOM+8zanxONmx4/p1bpGVlW6L1eKiH/imxZzTN99TANxC\n23nmTXgDAACYEm9YAgAAMOeENwAAgAYIbwAAAA0Q3gAAABogvAEAADRAeAMAAGiA8Abw/7d3RyGW\n3mcdx3+PZBeThg0oS8DE7trS0ESQGGVNjdIVjZoK6Y1S9SI0hFqkSYqCoEUwF96KtFQIwTW0EJfS\nYiVqCyp1CXqRrIlpa7NqtW6aRg0jNSltAknr34tzUieTmT1nJ+85k2fm84HlzLznvzMvf56Z5bvn\nPecAADQg3gAAABoQbwAAAA2INwAAgAbEGwAAQAPiDQAAoAHxBgAA0IB4AwAAaEC8AQAANCDeAAAA\nGhBvAFycc+eSj3xkdvt6sbGRnD07uwWAfUq8AbC8u+5Krrsuefe7Z7d33bXXZ5ScPp0cO5bcfPPs\n9vTpvT4jAFiJGmOs75tVjXV+PwAmdO7cLNi2euKJ5Npr138+yeyRtmPHkhde+P9jl16aPPlkcvTo\n3pwTAFxAVWWMUbv5ux55A2A5jzxyccfX4fz55PDhVx47dGh2HAD2GfEGwHJOnLi44+tw/Hjy4ouv\nPPbSS7PjALDPiDcAlnPttcmdd77y2J137t0lk8ns0shTp2aXSh45Mrs9dcolkwDsS57zBsDFOXdu\ndqnkiRN7G26bbWzMLpU8fly4AfC69lqe8ybeAAAA1sQLlgAAAOxz4g0AAKAB8QYAANCAeAMAAGhA\nvAEAADQg3gAAABoQbwAAAA2INwAAgAbEGwAAQAPiDQAAoAHxBgAA0IB4AwAAaEC8AQAANCDeAAAA\nGhBvAAAADYg3AACABsQbAABAA+INAACgAfEGAADQgHgDAABoQLwBAAA0IN4AAAAaEG8AAAANiDcA\nAIAGxBsAAEAD4g0AAKAB8QYAANCAeAMAAGhAvAEAADQg3gAAABoQbwAAAA2INwAAgAbEGwAAQAPi\nDQAAoAHxBgAA0IB4AwAAaEC8AQAANCDeAAAAGhBvAAAADSyMt6q6uqo+U1VfqKrPV9XdO6z7UFV9\nsaoer6rrpz9VAACAg2uZR96+meTXxxjfn+RtSd5XVW/dvKCqbkny5jHGW5K8N8m9k58pdLCxkZw9\nO7sFAIAJLYy3McZ/jTEen3/89STnkly1Zdk7k3x0vubhJFdU1ZUTnyu8vp0+nRw7ltx88+z29Om9\nPiMAAPaRi3rOW1UdT3J9koe33HVVkqc2ff50Xh14sH9tbCR33JG88ELy3HOz2zvu8AgcAACTuWTZ\nhVV1eZJPJHn//BG4Xbnnnnu+/fHJkydz8uTJ3X4peP04fz45fHgWbS87dGh2/OjRvTorAAD22Jkz\nZ3LmzJlJvlaNMRYvqrokyZ8n+fQY44Pb3H9vkr8ZY3xs/vk/JXn7GOOZLevGMt8P2tnYmF0quTne\nLr00efJJ8QYAwLdVVcYYtZu/u+xlk3+U5Intwm3uwSS3zU/mxiTPbg032NeOHk1OnZoF25Ejs9tT\np4QbAACTWfjIW1XdlOShJJ9PMuZ/PpDkWJIxxrhvvu7DSX42yTeS3D7GeGybr+WRN/a3jY3ZpZLH\njws3AABe5bU88rbUZZNTEW8AAMBBto7LJgEAANhD4g0AAKAB8QYAANCAeAMAAGhAvAEAADQg3gAA\nABoQbwAAAA2INwAAgAbEGwAAQAPiDQAAoAHxBgAA0IB4AwAAaEC8AQAANCDeAAAAGhBvAAAADYg3\nAACABsQbAABAA+INAACgAfEGAADQgHgDAABoQLwBAAA0IN4AAAAaEG8AAAANiDcAAIAGxBsAAEAD\n4g0AAKAB8QYAANCAeAMAAGhAvAEAADQg3gAAABoQbwAAAA2INwAAgAbEGwAAQAPiDQAAoAHxBgAA\n0IB4AwAAaEC8AQAANCDeAAAAGhBvAAAADYg3AACABsQbAABAA+INAACgAfEGAADQgHgDAABoQLwB\nAAA0IN4AAAAaEG8AAAANiDcAAIAGxBsAAEAD4g0AAKAB8QYAANCAeAMAAGhAvAEAADQg3gAAABoQ\nbwAAAA2INwAAgAbEGwAAQAPiDQAAoAHxBgAA0IB4AwAAaEC8AQAANCDeAAAAGhBvAAAADYg3AACA\nBsQbAABAA+INAACgAfEGAADQgHgDAABoQLwBAAA0IN4AAAAaEG8AAAANiDcAAIAGxBsAAEAD4g0A\nAKAB8QYAANCAeAMAAGhAvAEAADQg3gAAABoQbwAAAA2INwAAgAbEGwAAQAPiDQAAoAHxBgAA0IB4\nAwAAaEC8AQAANCDeAAAAGhBvAAAADYg3AACABsQbAABAA+INAACgAfEGAADQgHgDAABoQLwBAAA0\nIN4AAAAaEG8AAAANiDcAAIAGxBsAAEAD4g0AAKAB8QYAANCAeAMAAGhAvAEAADQg3gAAABoQbwAA\nAA2INwAAgAbEGwAAQAPiDQAAoAHxBgAA0IB4AwAAaEC8AQAANCDeAAAAGlgYb1V1qqqeqarP7XD/\n26vq2ap6bP7nt6c/TQAAgINtmUfe7k/yMwvWPDTGuGH+53cnOC8mcubMmb0+hQPHnq+fPV8/e75+\n9nz97Pn62fP1s+e9LIy3McbfJvmfBctqmtNhan4g18+er589Xz97vn72fP3s+frZ8/Wz571M9Zy3\nt1XV41X1F1V13URfEwAAgLlLJvgajyZ54xjj+aq6JcmfJrlmgq8LAADAXI0xFi+qOpbkz8YYP7DE\n2n9P8kNjjK9uc9/ibwYAALCPjTF29bSzZR95q+zwvLaqunKM8cz84xOZBeGrwu21nCQAAMBBtzDe\nquqPk5xM8t1V9eUkv5PkcJIxxrgvyc9X1a8meSnJC0netbrTBQAAOJiWumwSAACAvTXVq02+SlV9\nx/xNux/c4f4PVdUX569Sef2qzuMgudCeezP16VXV+ar6bFX9Q1U9ssMacz6hRXtuzqdXVVdU1cer\n6lxVfaGqfmSbNeZ8Qov23JxPq6qumf9OeWx++1xV3b3NOnM+kWX23JxPr6p+a/475XNV9UBVHd5m\njTmf0KI9382cT/Fqkzt5f5InkhzZesf8VSnfPMZ4y/wfpXuT3LjCczkodtzzuYfGGLeu8Xz2u/9N\ncnKMse37IJrzlbjgns+Z82l9MMmnxhi/UFWXJLls853mfCUuuOdz5nwiY4x/SfKDyew/QZN8Jckn\nN68x59NaZs/nzPlE5i8++J4kbx1jvFhVH0vyi0k+ummNOZ/QMns+d1FzvpJH3qrq6iTvSPKHOyx5\nZ+YnPsZ4OMkVVXXlKs7loFhizxNvpj61yoV/hsz59Bbt+ctrmEBVHUny42OM+5NkjPHNMcbXtiwz\n5xNacs8Tc74qP5Xk38YYT205bs5XZ6c9T8z5lL6W5MUkb9j0n0L/sWWNOZ/WMnueXOScr+qyyd9P\n8htJdnpC3VVJNv+QPj0/xu4t2vPEm6lPbST5q6o6W1Xv2eZ+cz69RXuemPMpfV+S/66q++eXc9xX\nVZduWWPOp7XMnifmfFXeleT0NsfN+erstOeJOZ/M/IqV30vy5czm99kxxl9vWWbOJ7TknicXOeeT\nx1tV/VySZ8YYj+cCbzHAdJbc85ffTP36JB/O7M3UeW1uGmPckNkjnu+rqh/b6xM6ABbtuTmf1iVJ\nbkjyB/N9fz7Jb+7tKe17y+y5OV+BqjqU5NYkH9/rczkoFuy5OZ9QVb0pya8lOZbke5JcXlW/vLdn\ntb8tuecXPeereOTtpiS3VtWXMvuflJ+oqq3Xdj6d5Hs3fX71/Bi7s3DPxxhfH2M8P//400kOVdV3\nrf9U948xxn/Obzcyu1b/xJYl5nxii/bcnE/uK0meGmP8/fzzT2QWFpuZ82kt3HNzvjK3JHl0/vtl\nK3O+GjvuuTmf3A8n+bsxxlfHGN9K8idJfnTLGnM+rYV7vps5nzzexhgfGGO8cYzxpsyelPeZMcZt\nW5Y9mOS2JKmqGzN7GPGZqc/loFhmzzdfs1wL3kydxarqsqq6fP7xG5L8dJJ/3LLMnE9omT0359Oa\nz+tTVXXN/NBPZvaiSJuZ8wkts+fmfGV+KTtfvmfOV2PHPTfnk/vnJDdW1XdWVWX2u+XcljXmfFoL\n9/LeF4MAAAC+SURBVHw3c77KV5t8hap6b+Zv7D3G+FRVvaOq/jXJN5Lcvq7zOEg273m8mfrUrkzy\nyaoamf0cPTDG+EtzvlIL9zzmfBXuTvLA/PKmLyW53Zyv3AX3POZ8clV1WWYvnPErm46Z8xVatOcx\n55MaY3x2flXWo0m+leSxJPeZ89VZZs+zizn3Jt0AAAANrOxNugEAAJiOeAMAAGhAvAEAADQg3gAA\nABoQbwAAAA2INwAAgAbEGwAAQAPiDQAAoIH/A//EDteMnLp7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d8212e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(class0[:, 0], class0[:, 1], color='red', marker='o')\n",
    "plt.scatter(class1[:, 0], class1[:, 1], color='blue', marker='x')\n",
    "plt.title('Setosa vs. Non-setosa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLogistic(data=iris_data_binary,\n",
    "                                test_sample_ratio=0.1,\n",
    "                                learning_rate=0.1,\n",
    "                                regularization_lambda=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic trainer setup.\n",
      "Initializing......\n",
      "Setting up trainer......\n",
      "Training......\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (3,) doesn't match the broadcast shape (2,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-dc13262d806d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mregression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-a5dfc8626d1a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, time_limit, iteration_limit, print_cost)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                       \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                       \u001b[0miteration_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                       print_cost_while_training=print_cost)\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setup_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-34200d2415f4>\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, training_algorithm, learning_rate, time_limit, iteration_limit, print_cost_while_training)\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0miteration_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     print_cost_while_training)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__print_end_training_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f2d5fb95709b>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_algorithm, learning_rate, time_limit, iteration_limit, print_cost_while_training)\u001b[0m\n\u001b[1;32m     54\u001b[0m                print_cost_while_training=False):\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_algorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRegressionAlgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_with_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost_while_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot start training, no logistic regression algorithm specified.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-34200d2415f4>\u001b[0m in \u001b[0;36m_train_with_gradient_descent\u001b[0;34m(self, learning_rate, time_limit, iteration_limit, print_cost_while_training)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# If the cost hasn't changed in 20 (2 * 10) iterations, it converged.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_theta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_derivative_of_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0;31m# Check and print cost every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcost_check_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (3,) doesn't match the broadcast shape (2,3)"
     ]
    }
   ],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train(time_limit=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Written Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Data\n",
    "mat_data = scipy.io.loadmat('digits_data/hand_written_digits.mat')\n",
    "data = np.append(mat_data['X'], mat_data['y'], axis=1)\n",
    "np.random.shuffle(data)\n",
    "if data is not None:\n",
    "    print('Successfully queried data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize Setup\n",
    "setup = RegressionSetupLogistic(data=data[0:100],\n",
    "                                test_sample_ratio=0.1,\n",
    "                                learning_rate=0.1,\n",
    "                                regularization_lambda=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize Regression\n",
    "regression = Regression(setup)\n",
    "regression.train(time_limit=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
